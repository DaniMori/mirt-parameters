---
title:     |
  Parámetros multidimensionales en Teoría de Respuesta al Ítem:
  ¡Algo estamos haciendo mal!
author:    |
  <table>
    <tr>
      <td>Daniel Morillo, Ph.D.</td>
    </tr>
    <tr>
      <td>
        <center>
          [![](../www/slideshow-assets/github-logo.png){height="50"}](https://github.com/DaniMori/)&emsp;
          [![](../www/slideshow-assets/orcid.png){height="50"}](https://orcid.org/0000-0003-3021-3878)
        </center>
      </td>
    </tr>
  </table>
institute: |
  <table>
    <tr>
      <td>
        <center>
          [![](../www/slideshow-assets/logos_fac_psicologia.svg){height="120"}](https://www.uned.es/universidad/facultades/psicologia.html)
        </center>
      </td>
    </tr>
  </table>
bibliography:  ../www/Multidimensional-parameters-MCLM.bib
csl:           ../www/apa-old-doi-prefix.csl
date:          "2024-10-21"
date-meta:     "2024-11-27"
date-format:   long
editor:
  mode:        source
  markdown: 
    wrap:      sentence
    canonical: true
lang:          es
knitr:
  opts_knit: 
    root_dir:  here::here()
  opts_chunk: 
    results:   asis
format:
  revealjs:
    auto-stretch:            true
    code-annotations:        hover
    df-print:                paged
    fig-cap-location:        bottom
    incremental:             false
    keep-md:                 true
    link-external-newwindow: true
    self-contained:          true
    slide-number:            false
    theme:                   ../www/slideshow-assets/extra-styles.scss
    transition:              none
    view-distance:           3
    template-partials:
      - ../www/slideshow-assets/title-slide.html
---

# Introducción

```{r libraries}
library(tibble)
library(dplyr, warn.conflicts = FALSE)
library(knitr)
```

::: notes
-   Nombre de la presentación: Cero atractivo

-   Nuria me pide un título más llamativo

-   Lo mejor que puedo conseguir

-   ¿Por qué es relevante?

-   TRI muy usada en evaluación e investigación educativa

-   Poco en investigación psicológica (muestras "pequeñas" en comparación)

-   Pero hay un "paralelismo" entre ambas teorías (isomórficas, ante determinados supuestos)
:::

<!-- Slides "Diferencias AF - TRI" -->

```{r irt-fa-table}
af_tri_differences <- tribble(
  ~AF,                            ~TRI,
  "Relaciones entre variables",   "Comparación entre participantes",
  "Cuestionarios",                "Pruebas (rendimiento)",
  "Explicación",                  "Medición",
  "Puntuación compuesta",         "Puntuación latente",
  "Estructura simple",            "Diferencias individuales"
)

section_title <- "## Diferencias AF - TRI"

for (slide in af_tri_differences |> nrow() |> seq_len()) {
  
  cat( "\n", section_title, sep = '')
  
  af_tri_differences |> slice_head(n = slide) |> kable() |> print()
}
```

## AF: Estructura simple

<!--# TODO: Ejemplos estructura simple vs. compleja: Path diagram vs. coordenadas -->

::: notes
Para darle más sentido al modelo factorial es común buscar la "estructura simple".
Es decir, asumimos que la varianza de cada ítem está explicada solamente por un factor latente.
Los pesos factoriales en los demás factores son nulos.
Si representamos los pesos factoriales como coordenadas, el ítem estará alineado con el eje correspondiente al factor en el que pesa.
:::

## AF: Estructura simple

<!--# TODO: Ejemplo mostrando solución factorial sin rotar y ejes tras rotación oblicua? -->

::: notes
Para obtener esa estructura simple, muchas veces necesitamos usar una rotación oblicua.
:::

## AF: Estructura compleja

<!--# TODO: Ejemplo con pesos cruzados: Path diagram vs. coordenadas -->

::: notes
Sin embargo, en ocasiones no es posible encontrar la estructura simple.
Es cuando aparecen lo que llamamos "pesos cruzados".
Esto da lugar a una "estructura compleja", que en realidad es más común que la estructura simple, en general.
:::

## AF: Estructura compleja

<!--# TODO: Ejemplo con factores de método: Path diagram vs. coordenadas -->

::: notes
Por ejemplo, si tenemos ítems directos e inversos, podemos modelar lo que llamamos "factores de método": Una variable latente para cada factor sustantivo, y una para cada uno de los tipos de ítem.
:::

## Modelo factorial clásico

<!--# TODO: Fórmula del modelo factorial -->

<!--# TODO: Fórmula matricial para una variable observable -->

::: notes
Un modelo factorial no es más que una serie de regresiones lineales (una por cada variable observable) con variables latentes.
Las variables "respuesta" o "criterio" es la variable observable, y su varianza está explicada o "predicha" por las variables latentes, que son los factores y los términos de error de cada ítem.

Los "coeficientes de regresión" los llamamos "pesos factoriales" y el término error, "unicidad" o "varianza única".
Esto se puede expresar en "forma matricial" de esta manera.

<!--# TODO: Explicar cómo se calcula en forma matricial? -->

Tenemos costumbre de ver el análisis factorial en el contexto de variables cuantitativas (o que asumimos que lo son, por ejemplo en ítems Likert).
Sin embargo, se aplica también (e idealmente se debería aplicar) a variables discretas ordinales.
En este caso vamos a concretarlo en variables dicotómicas (p.ej. ante un ítem como "Soy muy responsable" las opciones de respuesta son sólo V/F).
Esto es una simplificación que rara vez se utiliza en cuestionarios no cognitivos, pero es interesante estudiarlo porque otros modelos pueden considerarse extensiones de este modelo.
:::

## Análisis factorial de ítems

<!--# TODO: Fórmula del modelo factorial con función de enlace probit -->

<!--# TODO: Función de enlace "normal acumulada" -->

<!--# TODO: Curva Gaussiana y curva normal acumulada -->

::: notes
El análisis factorial de ítems (dicotómicos) lo que hace es aplicar ese "modelo lineal general" a variables dicotómicas: Asume una "variable latente" lineal, con la misma fórmula anterior, y aplica una "función de umbral" en 0 (la respuesta es correcta o afirmativa, codificada como 1, si la variable latente es mayor que 0, o incorrecta o negativa, codificada como 0, si es menor).
Eso da una "probabilidad" de responder correcta o "afirmativamente" a un ítem.
:::

## AF: Puntuaciones factoriales

Métodos:

-   Regresión

-   Bartlett

-   **Máxima Verosimilitud**

-   **Estimación Modal Bayesiana Empírica**

<!--# TODO: Diagrama de CFA y de "CFA inverso" -->

::: notes
Los dos primeros métodos se aplican en AF lineal y están implementados en SPSS.
Consisten a grandes rasgos en calcular la puntuación factorial como una suma ponderada de las variables observables.
Es decir, algo así como "darle la vuelta a las flechas" de causalidad.
Lo que tiene más sentido, desde el punto de vista estadístico, es "estimar" qué puntuaciones factoriales hacen más probable para una participante dar las respuestas que ha dado.

Eso es lo que hacen los dos métodos de abajo (SPSS no los implementa creo), Máxima Verosimilitud, o esimación Modal Bayesiana.
Si habéis hecho esto con variables categóricas, entonces habéis aplicado la Teoría de Respuesta al Ítem, y a continuación veremos por qué.
:::

## Paralelismo AF - TRI

::::: columns
::: column
![](../www/slideshow-assets/cover_McDonald-1999.jpg){height="300px"}

[@mcdonald_test_1999]
:::

::: column
<!--# TODO: Citation by McDonald -->
:::
:::::

## Teoría de respuesta al ítem

<!--# TODO: Fórmula de modelo 2PL -->

<!--# TODO: Gráfica de discriminación alta/baja, dificultad alta/baja -->

<!--# TODO: Fórmula del parámetro de intersección y del modelo en forma pendiente-intersección -->

::: notes
La TRI, por su lado, asume que hay cada persona tiene una puntuación latente, y que su respuesta depende de esa puntuación, con un parámetro de discriminación que determina cómo de bien o mal mide el ítem, y uno de dificultad (aunque aquí prefiero llamarlo "de posición") que determina lo fácil o difícil que será dar una respuesta positiva (afirmativa, correcta, codificada como 1).
Esa posición representa el nivel de rasgo latente que da lugar a una probabilidad de .5 de dar la respuesta positiva.

Ahora bien, la posición se puede reformular como un parámetro de intersección.

Como se puede ver, esto también se trata de un "modelo lineal general".
En este caso, en lugar de una función probit (normal acumulada), la función de enlace es la función "logit".
Es decir, la respuesta a cada ítem se modela como una regresión logistica donde los predictores son las variables latentes.
:::

## Teoría de respuesta al ítem multidimensional

<!--# TODO: Fórmula de modelo M2PL -->

<!--# TODO: Gráfica tridimensional de modelo compensatorio (y "curvas de nivel"?) -->

::: notes
Una de las extensiones multidimensionales (más antiguas, y probablemente más obvias) de este modelo consiste simplemente en asumir que la respuesta se modela como una "regresión logística multivariada".

Esto da lugar a una familia de modelos llamados "compensatorios" en TRI, ya que un nivel bajo en una puntuación latente se compensa con un nivel alto en otra, dando lugar a la misma probabilidad de respuesta.
:::

## TRI multidimensional: Ejemplo

<!--# TODO: Valores, cálculo y gráfica -->

::: notes
Veamos un ejemplo de cuál sería la probabilidad de respuesta de una persona con puntuaciones de rasgo $\theta_1 = TODO$ y $\theta_2 = TODO$.

(Hacer cálculos matriciales)
:::

## AF en métrica TRI

<!--# TODO: Fórmulas de los modelos y de transformación de parámetros. -->

::: notes
A partir de las ecuaciones del AF y de TRI, es fácil ver que existe un paralelismo entre ambos modelos.
Es posible transformar los parámetros de la métrica del AF a métrica de TRI y, de hecho, esto es necesario, y se hace así para hacer la estimación máximo-verosimil de la que hablábamos antes.
:::

## TRI Multidimensional: Parámetros multidimensionales

<!--# TODO: Fórmula de modelo M2PL -->

@reckase_difficulty_1985:

> \[...\] means of describing the characteristics of an item that takes into account the dimensionality of the skills \[...\] can then be used to determine how or if it is possible to compare items that measure different combinations of abilities.

## Dificultad multidimensional

> \[...\] the most reasonable point to use in defining the MID for an item in the multidimensional space is the point where the item is most discriminating.

---(p. 402)

<!--# TODO: Fórmula del parámetro D_i -->

> The distance can be interpreted much like a $b$ parameter from unidimensional IRT

---(p. 405)

<!--# TODO: Representación de la curva de inflexión y la posición del ítem -->

::: notes
A pesar de este paralelismo, y de la utilidad de los modelos de TRI, hay un problema con la formulación del modelo que os he planteado, y es que como habéis podido ver los parámetros no son fácilmente interpretables.
La intersección del modelo no tiene una interpretación clara, al contrario que el parámetro de posición en la TRI unidimensional, que permite comparar ítems entre sí en cuanto a su dificultad.

Esto en AF no es muy habitual (prestamos poca atención a la "intersección" o "umbral"), porque no nos interesa tanto cómo de probable sea dar una respuesta positiva o negativa a un ítem, pero podría ayudarnos por ejemplo a diagnosticar ítems que estén mal diseñados, ya que nos da una idea de si va a haber muchas o pocas respuestas positivas a un ítem.

Por ese motivo Reckase propone en 1985 obtener un parámetro de "dificultad" (lo llama, para nosotro sería de posición) que permita comparar ítems, y que sea de algún modo equivalente a la dificultad en el modelo unidimensional.
Ese punto es donde la curva (superficie) de probabilidad de respuesta tiene una inflexión (a lo largo de toda ella, el punto más cercano al origen, donde la recta que lo une al origen es perpendicular a la recta de inflexión de la curva).

Este parámetro se puede entender como una "distancia con signo" desde el origen hasta el punto de posición del ítem.
:::

## Dificultad multidimensional

<!--# TODO: Fórmula de los cosenos directores -->

<!--# TODO: Representación gráfica de los cosenos directores? -->

<!--# TODO: Conjunto del MID -->

::: notes
Para representar ese punto no basta con la "distancia (con signo)"; hace falta saber en qué dirección mide ese ítem también.
Eso viene dado por los llamados "cosenos directores".

Por lo tanto, la "dificultad (posición) multidimensional" viene dada por dos elementos, un vector de cosenos que determina la dirección en la que mide un ítem, y un número entero que representa cuánto se desplaza respecto del origen, y en qué sentido, la recta de inflexión del ítem.
Así se puede interpretar un ítem, o varios, de manera relativamente sencilla: Todos los participantes cuyas coordenadas estén en la "recta de inflexión" (la recta que pasa por la posición del ítem, perpendicular a su dirección) tendrán una probabilidad de dar una respuesta positiva y negativa de .5 (dificultad media; igual probabilidad de acertar o fallar).
Aquellos que estén pasada la recta, encontrarán el ítem "relativamente fácil" (tendrán una alta probabilidad de dar una respuesta positiva mayor a .5; lo encontrarán "más fácil que difícil").
Lo contrario pasa para los que estén al otro lado de esa recta, tendrán más probabilidad de responder negativamente (mal), por lo que lo encontrarán difícil.

De lo que este parámetro no informa es de "cómo de fácil o difícil" va a ser el ítem para aquellos participantes que están muy cerca o muy lejos de esta recta.
Es decir, una vez estoy pasada la recta, aunque esté cerca, ¿es el ítem muy fácil, o más bien tengo que estar "muy lejos" (tener alta puntuación en uno u otro rasgo) para que sea más probable que de una respuesta positiva?
:::

## Discriminación multidimensional

@reckase_discriminating_1991:

> the discriminating power of an item indicates how quickly the transition takes place from low probability to high probability of a correct response.
> \[...\] A highly discriminating item divides the regions clearly-having a narrow region of ambiguity, that is, a region where the probabilities are intermediate in magnitude.

<!-- # TODO: Superficies de ítems con alta y baja discriminación -->

::: notes
Esta característica viene representada por el parámetro de "discriminación", que es una medida de la "calidad general" de un ítem.
Los ítems "buenos" serán muy discriminativos, lo que indica que distinguen bien entre la gente que tiene nivel de rasgo alto o bajo, mientras que los "malos" tendrán una superficie de respuesta más "plana".

La discriminación se puede entender con una especie de "dardo" que atraviesa de arriba abajo esa superficie perpendicularmente por esa "recta de inflexión" de la que hablábamos antes, y luego cruza el plano origen.
Si la superficie es "totalmente plana" el dardo entra por un punto y sale por otro que está justo debajo, en las mismas coordenadas de rasgo latente.
La "proyección" (si ponemos un foco justo encima, la sombra que forma en el plano) es un punto.
Es decir, la "discriminación" es cero (el vector del ítem tiene longitud "cero").
Si la pendiente es muy pronunciada (distingue bien entre niveles de rasgo) el dardo atravesará el plano origen muy lejos.
La sombra será muy larga (es decir, la discriminación muy alta).
El caso "extremo" es una superficie "vertical" en la recta de inflexión (un escalón; cosa imposible en ítems reales).
En este caso, el dardo nunca cortaría al plano origen, y la discriminación sería infinita.
:::

## Parámetros multidimensionales

@reckase_discriminating_1991:

<!-- # TODO: Fórmula de MDISC -->

<!-- # TODO: Gráfico del cálculo de MDISC como teorema de Pitágoras -->

<!-- # TODO: Fórmula de dificultad con MDISC -->

::: notes
@reckase_discriminating_1991 hacen la deducción de la discriminación multidimensional, y resulta dar lugar a esta fórmula.
Como veis es simplemente la "longitud" del vector de parámetros de discriminación, resultante simplemente de aplicar el teorema de Pitágoras.
El vector es un triángulo rectángulo, $a_1$ sería un cateto, el otro cateto sería igual a $a_2$, y la hipotenusa es la longitud (las dos discriminaciones al cuadrado, sumadas, y luego la raíz cuadrada de eso).
Esto por cierto también vale para cualquier número de dimensiones.
Aquí estamos representando sólo 2 porque es más fácil, pero podemos calcular la suma de cuadrados estos parámetros de discriminación (sea cual sea su número), y calcular su raíz cuadrada, y eso nos daría igualmente la discriminación multidimensional (la longitud de ese vector de $a$ del ítem).

Esto es en realidad el producto escalar (o producto punto) de un vector consigo mismo, que consiste en multiplicar una a una las componentes y sumarlos todos.
En forma matricial se representaría como el vector pre-multiplicado por sí mismo.
<!-- # TODO: Explicar la multiplicación vectorial (y poner ejemplo?) -->

Esta expresión de la discriminación multidimensional se puede usar para expresar de manera más sencilla la dificultad.
:::

## Representación gráfica

@ackerman_graphical_1996:

::: notes
La propuesta de representación gráfica completa (y consistente) no llega hasta 1996.
Ackerman propone que el ítem se represente como un vector de longitud dada por la discriminación multidimensional, aplicado en la posición del ítem (y en la dirección dada por esa posición; es decir, todos los ítems "apuntan" siempre en la dirección que pasa por el origen).
:::

## Supuestos: Monotonía creciente

> the probability of answering an item correctly increases monotonically with an increase in each dimension being measured

---@reckase_difficulty_1985 [p. 402]

<!-- # TODO: Ecuación modelo y ejemplo probabilidad constante -->

::: notes
Ahora bien, como hemos visto, la TRI originalmente sólo trataba con "ítems cognitivos" (lo hemos podido ver en el propio lenguaje que utiliza Reckase: "dificultad", "habilidad", "respuesta correcta").
Uno de los supuestos que hace es este.
Eso tiene todo el sentido (casi), ya que si la probabilidad de un ítem "decrece" al aumentar el nivel de habilidad, es que algo está mal en el ítem: No puede ser que alguien que sepa más tenga menos probabilidad de acertar un ítem que alguien que sepa menos.
Pero cuidado, si el ítem es "puro" de una dimensión, la probabilidad en la otra u otras dimensiones sería constante (discriminación = 0); es decir, la probabilidad puede ser "plana" (es una dimensión que no "está siendo medida por el ítem").
:::

## Generalización: Ítems inversos

<!-- # TODO: Curva de ítem inverso y ejemplo -->

<!-- # TODO: Fórmulas de los parámetros -->

<!-- # TODO: Ejemplo con a negativa -->

::: notes
Pero además, si hablamos de ítems "no cognitivos" (p.ej. personalidad) en los que la persona evaluada puede estar "de acuerdo" o "en desacuerdo", sí podemos tener ítems con probabilidad "decreciente".

La generalización a ítems inversos es directa: La discriminación multidimensional "eleva al cuadrado" los "parámetros de discriminación marginales", luego va a ser siempre positiva.
El parámetro de posición, por tanto, siempre va a tener el signo inverso al que tenga la intersección.
Los cosenos tendrán, cada uno, el mismo que el parámetro de discriminación de esa dimensión.
:::

## Generalización: Ítems inversos

<!-- # TODO: Representación de ítems con a (una o varias) negativa (tabla y gráfico) -->

::: notes
La representación de estos ítems sería como se muestra en este gráfico.
<!-- # TODO: Descripción de los ítems representados -->
:::

## Ítems "inversos" en la literatura

> the negative relation of Item 15 with geometry achievement is particularly puzzling

---@mcdonald_basis_2000 [p. 109]

> This result occurs because $a_1$ for the item is negative, and thus as an examinee's $\theta_1$ ability increases, the chance of answering the item correctly actually decreases.

---@ackerman_multidimensional_2005-1 [p. 17]

### Generalización: "Monotonía constante"

::: notes
Esta generalización es bastante evidente, y se puede aplicar sin problema a ítems no cognitivos.
Sin embargo, hay que tener en cuenta que Reckase explicitaba como condición que los ítems tenían que ser "monotonamente crecientes".
Pero en la literatura nos encontramos casos como estos, que no lo son, y a los autores no se les ocurre decir que "hay que generalizar esas definiciones".
Más bien, hacen comentarios como estos, restándole importancia al hecho de que los parámetros de discriminación sean negativos ("particularmente misterioso"), o afirmando lo obvio (el parámetro de discriminación del ítem es negativo para una dimensión, por lo que la probabilidad de respuesta correcta disminuye al incrementarse su habilidad)

En lugar de eso, proponemos que los supuestos de Reckase se "relajen" para considerar ítems con probabilidad "con monotonía constante" (es decir, que no tengan un "pico" u óptimo de rasgo latente para dar una respuesta positiva y luego decrezca), sino que cuanto más extremo sea el nivel de rasgo, más extrema es también la probabilidad (positiva o negativa).
Este caso general incluye también la probabilidad constante (ya que es monotonía "nula constante", es decir, "el cambio es nulo de manera constante").
Este tipo de ítems, positivos, negativos, o nulos, se pueden acomodar y modelar fácilmente por un modelo como el logístico de 2 parámetros multidimensional, como hemos visto.
Ítems que estarían excluidos, por no tener monotonía constante, podrían ser formulaciones como, por ejemplo, "Soy tan extrovertido como la mayoría de mis amigos" (alguien puede decir, "No, soy más introvertido que todos ellos", respuesta negativa con rasgo bajo, o "Qué va, soy el más extrovertido con diferencia", respuesta negativa con rasgo alto).
:::

## Supuestos: Ortogonalidad

> before performing the differentiation, the constraint that <!-- # TODO: Sum of cosines^2 = 1 --> is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

<!-- # TODO: Ejemplo de rotación oblicua con los ejes resultantes oblicuos -->

<!-- # TODO: Coordenadas rectangulares resultantes de una rotación oblicua -->

::: notes
Otro de los supuestos que hace Reckase originalmente para calcular los parámetros es este: Los cosenos al cuadrado suman 1.
Es decir, está aplicando el teorema de Pitágoras, pero ello implica de manera implícita que los ejes del espacio latente son "ortogonales" (forman ángulos de 90 grados).

Esto en TRI multidimensional originalmente tenía sentido, ya que no se tenía muy en cuenta la interpretación de los parámetros de rasgo latente.
O bien no importaba mucho que los ejes se pudieran interpretar como rasgos latentes, o se interpretaban de manera no necesariamente muy precisa.

No obstante, en 1999 llega McDonald, y dice que la TRI y el AF son fundamentalmente la misma cosa.
Pero en AF tenemos dimensiones correlacionadas la mayoría de las veces, y eso como hemos visto implica hacer rotaciones oblicuas que dan lugar a ejes no ortogonales.

Los pesos factoriales resultantes (o discriminaciones) los podemos representar sin ningún problema en coordenadas rectangulares (las llamadas "coordenadas Cartesianas").
Pero algo parece fallar cuando generalizamos los parámetros multidimensionales de TRI que hemos mostrado antes a esos espacios de coordenadas no ortogonales.
Si rotamos los ejes de coordenadas, pero los ítems siguen siendo los mismos, es de esperar que la "longitud" del vector de ese ítem (i.e. su discriminación), no cambie (esta propiedad se llama invarianza).
Veámoslo con un ejemplo.
:::

## Supuestos: Ortogonalidad

<!-- # TODO: Ejemplo de item en coordenadas ortogonales (gráfico y vectorial) -->

<!-- # TODO: Ejemplo de rotación oblicua (gráfico de ejes y matriz de rotación) -->

::: notes
Supongamos este ítem, en coordenadas ortogonales.
El vector muestra sus parámetros de discriminación en las dos dimensiones de los ejes representados.
Esto sería la "solución sin rotar" que resulta típicamente de ajustar un modelo de TRI multdimensional, o un AFE exploratorio.
La discriminación multidimensional de este vector es (explicar paso a paso).

Supongamos que aplicamos una rotación oblicua.
Hemos identificado dos rasgos latentes sustantivos, correlacionados entre sí.
Los ejes resultantes son los que se muestran en este gráfico.
Los nuevos parámetros de discriminación del ítem están representadas cada una por la "proyección" de ese vector sobre cada uno de los ejes (paralela al otro, u otros, si hay más de dos).
:::

## Supuestos: Ortogonalidad

<!-- # TODO: Ejemplo de item en coordenadas ortogonales (gráfico y vectorial) -->

<!-- # TODO: Ejemplo de item proyectado en los ejes rotados (gráfico y vectorial) -->

<!-- # TODO: Cálculo de la discriminación en los ejes rotados -->

::: notes
Hacer una rotación supone "multiplicar" las coordenadas de este vector por la matriz de rotación que representa ese nuevo sistema de coordenadas (la transformación del ortogonal a este nuevo, oblicuo).
En este caso, esta es la matriza de rotación.
Como los vectores se representan en columnas, se "pre-multiplica" por la matriz de rotación.
Esto es parecido a multiplicar por un vector; es como si cada fila de la matriz fuese un vector, y los resultados se ponen cada uno en una fila.
<!-- # TODO: Explicar paso a paso el producto matricial -->

Si ahora calculamos la discriminación del ítem en estos ejes rotados, nos encontramos con este resultado.
Es decir, ¿que el ítem discrimina mejor o peor en función de qué ejes quiera yo considerar?
Eso no parece tener sentido, y sin embargo hay muchos ejemplos en la literatura que calculan esta discriminación multidimensional con la fórmula de Reckase (con "rasgos correlacionados") sin preocuparse por "los ejes" que representan esos rasgos de manera fidedigna.
<!-- # TODO: Ejemplos de la literatura -->
:::

## Supuestos: Ortogonalidad

> It has been suggested by some researchers that the angle between the axes represents the degree of correlation.
> However, for sake of clarity, an orthogonal axes system is used in which distance measures and vectors can be easily calculated, understood, and interpreted.

---@ackerman_multidimensional_2005-1 [p. 16]

### Generalización: "Coordenadas oblicuas"

::: notes
Para la representación gráfica esto puede valer, como dice Ackerman aquí.
Pero a la hora de calcular los parámetros (y Ackerman lo hace en este capítulo de libro), solamente considerar el caso ortogonal, la información que obtenemos resulta deficiente.
:::

## Generalización: Coordenadas oblicuas

> before performing the differentiation, the constraint that <!-- # TODO: Sum of cosines^2 = 1 --> is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

<!-- # TODO: Expresión de las \thetas transformadas -->

<!-- # TODO: Distribución de \thetas en ejes ortogonales -->

::: notes
Hacer esta generalización no es algo tan directo como ocurría con la probabilidad "monótonamente decreciente".
Si os acordáis, Reckase aplicaba esta restricción de que la suma de cosenos directores era 1 (que se deriva del teorema de Pitágoras).
Pero para aplicarla hace falta asumir que esos cosenos directores son todos ortogonales entre sí.
Si no tenemos ángulos rectos, esa condición no se cumple.

Y lo que nosotros estamos asumiendo que tenemos una distribución de vectores de rasgo latente ($\theta$) con una matriz de correlaciones ($R$, la que sea).
Normalmente representamos esos vectores en coordenadas rectangulares (ejes ortogonales), pero para obtener los parámetros multidimensionales generalizados a coordenadas oblicuas tenemos que representados en un espacio no ortogonal (aquí también hablaría de "unidades no estandarizadas", es decir, que la distancia entre el o y el 1 pueda ser distinta de la base estándar, pero digamos solamente "no ortogonal" por simplificar).

¿Cómo podríamos solucionar esto?
Lo que podemos hacer es aplicar una "transformación" a los rasgos latentes de forma que pasemos de este espacio oblicuo (el que sea, aún no sabemos cuál es) a un espacio ortogonal.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Expresión de las \thetas transformadas -->

<!-- # TODO: Expresión de la R transformada -->

<!-- # TODO: Distribución (esférica) de las \thetas transformadas -->

::: notes
Esto es una transformación de las coordenadas, como la que aplicamos antes para calcular las coordenadas del ítem tras aplicar la rotación oblicua, pero aplicada en este caso al vector de rasgo latente.

Estas nuevas coordenadas (transformadas), que ahora están en un espacio ortogonal, queremos que representen una distribución en la que las correlaciones sean nulas, es decir, que la matriz de correlaciones transformada ($R^B$) sea una matriz identidad (unos en la diagonal, ceros en el resto).
Pero hay que tener en cuenta que esto es un "artefacto matemático".
Estamos haciendo esto por conveniencia, pero las coordenadas que realmente nos interesan como investigadores, porque son las que verdaderamente representan algo que nos interesa investigar, son esas coordenadas originales, que pueden representar cualquier fenómeno que sea de nuestro interés (rasgos de personalidad, síntomas, actitudes, inteligencia, conocimientos... lo que estemos investigando).
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Ejemplo de \theta (expresión) y coordenadas en espacio oblicuo -->

::: notes
Mientras tanto, las coordenadas de los "rasgos latentes" originales se van a representar en otros ejes, en general distintos de los "ejes ortogonales" transformados.
Esto lo que significa es que las puntuaciones de rasgo latente originales van a representar la posición de un participante respecto de esos ejes originales.
Por ejemplo, en este caso, si el participante tiene un "vector de puntuaciones" \[1, 2\], para representar sus coordenadas tenemos que desplazarnos, primero una unidad en el primer eje, y luego dos unidades en el segundo eje.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Opciones de elección de bases oblicuas: rectangular, ejes muy alineados, ejes correctos -->

::: notes
En base a lo planteado anteriormente, me gustaría hacer una prueba con vosotras y vosotros.
Supongamos una distribución bivariada de rasgos latentes, con una correlación de .8, bastante alta. ¿Cuál de los siguientes sistemas de coordenadas consideráis que representaría adecuadamente los ejes de una distribución bivariada, de forma que al transformarlos a coordenadas rectangulares obtuviésemos una distribución con correlación de 0?
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Distribución en ejes rectangulares -->

::: notes
El primero ya sabemos que no, de acuerdo, puesto que es la representación habitual, y sabemos que la nube que forma está "achatada".
No se aplica ninguna transformación (la transformación es una matriz identidad), ya que los que serían los ejes originales y los transformados son los mismos.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Animación de distribución en ejes rectangulares a ejes alineados -->

::: notes
Si pensáis igual que yo, habréis elegido el segundo, porque el coseno del ángulo entre los ejes representa la correlación entre las dimensiones.
Por lo tanto, ejes muy alineados representan una correlación muy alta.

Sin embargo, fijáos lo que ocurre cuando "transformamos" las coordenadas de los puntos de nuestra distribución a ese sistema de coordenadas.
Como podéis ver, la distribución tiende a "achatarse" aún más, en lugar de hacerse más "esférica".
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Animación de distribución en ejes rectangulares a ejes correctos -->

<!-- # TODO: Matriz de correlaciones transformada "identidad" -->

::: notes
El sistema de coordenadas correcto es el tercero, que "compensa" de alguna manera el achatamiento de la nube de puntos, de forma que las coordenadas transformadas que nos quedan (en los ejes ortogonales), los "normales" del gráfico) tienen una distribución "esférica" (con matriz de correlaciones "identidad")
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Fórmula del modelo en coordenadas "transformadas" y equivalencia -->

<!-- # TODO: Expresión de la transformación de la discriminación a coordenadas rectangulares -->

::: notes
Lo que ocurre es que el espacio en el que se encuentran estas coordenadas de rasgo latente es diferente del espacio en el que se encuentran los ítems.
Para que el modelo tenga una probabilidad "invariante", los ítems tamibén tienen que transformarse a un sistema de coordenadas ortogonal.
Pero podemos ver que su matriz de transformación no es la misma que la de las puntuaciones de rasgo latente.
Es su inversa (inversa traspuesta, más concretamente)
:::

## References {.smaller}
