---
title:     |
  Parámetros multidimensionales en Teoría de Respuesta al Ítem:
  ¡Algo estamos haciendo mal!
author:    |
  <table>
    <tr>
      <td>Daniel Morillo, Ph.D.</td>
    </tr>
    <tr>
      <td>
        <center>
          [![](../www/slideshow-assets/github-logo.png){height="50"}](https://github.com/DaniMori/)&emsp;
          [![](../www/slideshow-assets/orcid.png){height="50"}](https://orcid.org/0000-0003-3021-3878)
        </center>
      </td>
    </tr>
  </table>
institute: |
  <table>
    <tr>
      <td>
        <center>
          [![](../www/slideshow-assets/logos_fac_psicologia.svg){height="120"}](https://www.uned.es/universidad/facultades/psicologia.html)
        </center>
      </td>
    </tr>
  </table>
bibliography:  ../www/Multidimensional-parameters-MCLM.bib
csl:           ../www/apa-old-doi-prefix.csl
date:          "2024-10-21"
date-meta:     "2024-11-27"
date-format:   long
editor:
  mode:        source
  markdown: 
    wrap:      sentence
    canonical: true
lang:          es
knitr:
  opts_knit: 
    root_dir:  here::here()
  opts_chunk: 
    results:   asis
format:
  revealjs:
    auto-stretch:            true
    code-annotations:        hover
    df-print:                paged
    fig-cap-location:        bottom
    incremental:             false
    keep-md:                 true
    link-external-newwindow: true
    self-contained:          true
    slide-number:            false
    theme:                   ../www/slideshow-assets/extra-styles.scss
    transition:              none
    view-distance:           3
    template-partials:
      - ../www/slideshow-assets/title-slide.html
---

# Introducción

```{r libraries}
library(tibble)
library(dplyr, warn.conflicts = FALSE)
library(knitr)
```

::: notes
-   Nombre de la presentación: Cero atractivo

-   Nuria me pide un título más llamativo

-   Lo mejor que puedo conseguir

-   ¿Por qué es relevante?

-   TRI muy usada en evaluación e investigación educativa

-   Poco en investigación psicológica (muestras "pequeñas" en comparación)

-   Pero hay un "paralelismo" entre ambas teorías (isomórficas, ante determinados supuestos)
:::

<!-- Slides "Diferencias AF - TRI" -->

```{r irt-fa-table}
af_tri_differences <- tribble(
  ~AF,                            ~TRI,
  "Relaciones entre variables",   "Comparación entre participantes",
  "Cuestionarios",                "Pruebas (rendimiento)",
  "Explicación",                  "Medición",
  "Puntuación compuesta",         "Puntuación latente",
  "Estructura simple",            "Diferencias individuales"
)

section_title <- "## Diferencias AF - TRI"

for (slide in af_tri_differences |> nrow() |> seq_len()) {
  
  cat( "\n", section_title, sep = '')
  
  af_tri_differences |> slice_head(n = slide) |> kable() |> print()
}
```

## AF: Estructura simple

<!--# TODO: Ejemplos estructura simple vs. compleja: Path diagram vs. coordenadas -->

::: notes
Para darle más sentido al modelo factorial es común buscar la "estructura simple".
Es decir, asumimos que la varianza de cada ítem está explicada solamente por un factor latente.
Los pesos factoriales en los demás factores son nulos.
Si representamos los pesos factoriales como coordenadas, el ítem estará alineado con el eje correspondiente al factor en el que pesa.
:::

## AF: Estructura simple

<!--# TODO: Ejemplo mostrando solución factorial sin rotar y ejes tras rotación oblicua? -->

::: notes
Para obtener esa estructura simple, muchas veces necesitamos usar una rotación oblicua.
:::

## AF: Estructura compleja

<!--# TODO: Ejemplo con pesos cruzados: Path diagram vs. coordenadas -->

::: notes
Sin embargo, en ocasiones no es posible encontrar la estructura simple.
Es cuando aparecen lo que llamamos "pesos cruzados".
Esto da lugar a una "estructura compleja", que en realidad es más común que la estructura simple, en general.
:::

## AF: Estructura compleja

<!--# TODO: Ejemplo con factores de método: Path diagram vs. coordenadas -->

::: notes
Por ejemplo, si tenemos ítems directos e inversos, podemos modelar lo que llamamos "factores de método": Una variable latente para cada factor sustantivo, y una para cada uno de los tipos de ítem.
:::

## Modelo factorial clásico

<!--# TODO: Fórmula del modelo factorial -->

<!--# TODO: Fórmula matricial para una variable observable -->

::: notes
Un modelo factorial no es más que una serie de regresiones lineales (una por cada variable observable) con variables latentes.
Las variables "respuesta" o "criterio" es la variable observable, y su varianza está explicada o "predicha" por las variables latentes, que son los factores y los términos de error de cada ítem.

Los "coeficientes de regresión" los llamamos "pesos factoriales" y el término error, "unicidad" o "varianza única".
Esto se puede expresar en "forma matricial" de esta manera.

<!--# TODO: Explicar cómo se calcula en forma matricial? -->

Tenemos costumbre de ver el análisis factorial en el contexto de variables cuantitativas (o que asumimos que lo son, por ejemplo en ítems Likert).
Sin embargo, se aplica también (e idealmente se debería aplicar) a variables discretas ordinales.
En este caso vamos a concretarlo en variables dicotómicas (p.ej. ante un ítem como "Soy muy responsable" las opciones de respuesta son sólo V/F).
Esto es una simplificación que rara vez se utiliza en cuestionarios no cognitivos, pero es interesante estudiarlo porque otros modelos pueden considerarse extensiones de este modelo.
:::

## Análisis factorial de ítems

<!--# TODO: Fórmula del modelo factorial con función de enlace probit -->

<!--# TODO: Función de enlace "normal acumulada" -->

<!--# TODO: Curva Gaussiana y curva normal acumulada -->

::: notes
El análisis factorial de ítems (dicotómicos) lo que hace es aplicar ese "modelo lineal general" a variables dicotómicas: Asume una "variable latente" lineal, con la misma fórmula anterior, y aplica una "función de umbral" en 0 (la respuesta es correcta o afirmativa, codificada como 1, si la variable latente es mayor que 0, o incorrecta o negativa, codificada como 0, si es menor).
Eso da una "probabilidad" de responder correcta o "afirmativamente" a un ítem.
:::

## AF: Puntuaciones factoriales

Métodos:

-   Regresión

-   Bartlett

-   **Máxima Verosimilitud**

-   **Estimación Modal Bayesiana Empírica**

<!--# TODO: Diagrama de CFA y de "CFA inverso" -->

::: notes
Los dos primeros métodos se aplican en AF lineal y están implementados en SPSS.
Consisten a grandes rasgos en calcular la puntuación factorial como una suma ponderada de las variables observables.
Es decir, algo así como "darle la vuelta a las flechas" de causalidad.
Lo que tiene más sentido, desde el punto de vista estadístico, es "estimar" qué puntuaciones factoriales hacen más probable para una participante dar las respuestas que ha dado.

Eso es lo que hacen los dos métodos de abajo (SPSS no los implementa creo), Máxima Verosimilitud, o esimación Modal Bayesiana.
Si habéis hecho esto con variables categóricas, entonces habéis aplicado la Teoría de Respuesta al Ítem, y a continuación veremos por qué.
:::

## Paralelismo AF - TRI

::::: columns
::: column
![](../www/slideshow-assets/cover_McDonald-1999.jpg){height="300px"}

[@mcdonald_test_1999]
:::

::: column
<!--# TODO: Citation by McDonald -->
:::
:::::

## Teoría de respuesta al ítem

<!--# TODO: Fórmula de modelo 2PL -->

<!--# TODO: Gráfica de discriminación alta/baja, dificultad alta/baja -->

<!--# TODO: Fórmula del parámetro de intersección y del modelo en forma pendiente-intersección -->

::: notes
La TRI, por su lado, asume que hay cada persona tiene una puntuación latente, y que su respuesta depende de esa puntuación, con un parámetro de discriminación que determina cómo de bien o mal mide el ítem, y uno de dificultad (aunque aquí prefiero llamarlo "de posición") que determina lo fácil o difícil que será dar una respuesta positiva (afirmativa, correcta, codificada como 1).
Esa posición representa el nivel de rasgo latente que da lugar a una probabilidad de .5 de dar la respuesta positiva.

Ahora bien, la posición se puede reformular como un parámetro de intersección.

Como se puede ver, esto también se trata de un "modelo lineal general".
En este caso, en lugar de una función probit (normal acumulada), la función de enlace es la función "logit".
Es decir, la respuesta a cada ítem se modela como una regresión logistica donde los predictores son las variables latentes.
:::

## Teoría de respuesta al ítem multidimensional

<!--# TODO: Fórmula de modelo M2PL -->

<!--# TODO: Gráfica tridimensional de modelo compensatorio (y "curvas de nivel"?) -->

::: notes
Una de las extensiones multidimensionales (más antiguas, y probablemente más obvias) de este modelo consiste simplemente en asumir que la respuesta se modela como una "regresión logística multivariada".

Esto da lugar a una familia de modelos llamados "compensatorios" en TRI, ya que un nivel bajo en una puntuación latente se compensa con un nivel alto en otra, dando lugar a la misma probabilidad de respuesta.
:::

## TRI multidimensional: Ejemplo

<!--# TODO: Valores, cálculo y gráfica -->

::: notes
Veamos un ejemplo de cuál sería la probabilidad de respuesta de una persona con puntuaciones de rasgo $\theta_1 = TODO$ y $\theta_2 = TODO$.

(Hacer cálculos matriciales)
:::

## AF en métrica TRI

<!--# TODO: Fórmulas de los modelos y de transformación de parámetros. -->

::: notes
A partir de las ecuaciones del AF y de TRI, es fácil ver que existe un paralelismo entre ambos modelos.
Es posible transformar los parámetros de la métrica del AF a métrica de TRI y, de hecho, esto es necesario, y se hace así para hacer la estimación máximo-verosimil de la que hablábamos antes.
:::

## TRI Multidimensional: Parámetros multidimensionales

<!--# TODO: Fórmula de modelo M2PL -->

@reckase_difficulty_1985:

> \[...\] means of describing the characteristics of an item that takes into account the dimensionality of the skills \[...\] can then be used to determine how or if it is possible to compare items that measure different combinations of abilities.

## Dificultad multidimensional

> \[...\] the most reasonable point to use in defining the MID for an item in the multidimensional space is the point where the item is most discriminating.

---(p. 402)

<!--# TODO: Fórmula del parámetro D_i -->

> The distance can be interpreted much like a $b$ parameter from unidimensional IRT

---(p. 405)

<!--# TODO: Representación de la curva de inflexión y la posición del ítem -->

::: notes
A pesar de este paralelismo, y de la utilidad de los modelos de TRI, hay un problema con la formulación del modelo que os he planteado, y es que como habéis podido ver los parámetros no son fácilmente interpretables.
La intersección del modelo no tiene una interpretación clara, al contrario que el parámetro de posición en la TRI unidimensional, que permite comparar ítems entre sí en cuanto a su dificultad.

Esto en AF no es muy habitual (prestamos poca atención a la "intersección" o "umbral"), porque no nos interesa tanto cómo de probable sea dar una respuesta positiva o negativa a un ítem, pero podría ayudarnos por ejemplo a diagnosticar ítems que estén mal diseñados, ya que nos da una idea de si va a haber muchas o pocas respuestas positivas a un ítem.

Por ese motivo Reckase propone en 1985 obtener un parámetro de "dificultad" (lo llama, para nosotro sería de posición) que permita comparar ítems, y que sea de algún modo equivalente a la dificultad en el modelo unidimensional.
Ese punto es donde la curva (superficie) de probabilidad de respuesta tiene una inflexión (a lo largo de toda ella, el punto más cercano al origen, donde la recta que lo une al origen es perpendicular a la recta de inflexión de la curva).

Este parámetro se puede entender como una "distancia con signo" desde el origen hasta el punto de posición del ítem.
:::

## Dificultad multidimensional

<!--# TODO: Fórmula de los cosenos directores -->

<!--# TODO: Representación gráfica de los cosenos directores? -->

<!--# TODO: Conjunto del MID -->

::: notes
Para representar ese punto no basta con la "distancia (con signo)"; hace falta saber en qué dirección mide ese ítem también.
Eso viene dado por los llamados "cosenos directores".

Por lo tanto, la "dificultad (posición) multidimensional" viene dada por dos elementos, un vector de cosenos que determina la dirección en la que mide un ítem, y un número entero que representa cuánto se desplaza respecto del origen, y en qué sentido, la recta de inflexión del ítem.
Así se puede interpretar un ítem, o varios, de manera relativamente sencilla: Todos los participantes cuyas coordenadas estén en la "recta de inflexión" (la recta que pasa por la posición del ítem, perpendicular a su dirección) tendrán una probabilidad de dar una respuesta positiva y negativa de .5 (dificultad media; igual probabilidad de acertar o fallar).
Aquellos que estén pasada la recta, encontrarán el ítem "relativamente fácil" (tendrán una alta probabilidad de dar una respuesta positiva mayor a .5; lo encontrarán "más fácil que difícil").
Lo contrario pasa para los que estén al otro lado de esa recta, tendrán más probabilidad de responder negativamente (mal), por lo que lo encontrarán difícil.

De lo que este parámetro no informa es de "cómo de fácil o difícil" va a ser el ítem para aquellos participantes que están muy cerca o muy lejos de esta recta.
Es decir, una vez estoy pasada la recta, aunque esté cerca, ¿es el ítem muy fácil, o más bien tengo que estar "muy lejos" (tener alta puntuación en uno u otro rasgo) para que sea más probable que de una respuesta positiva?
:::

## Discriminación multidimensional

@reckase_discriminating_1991:

> the discriminating power of an item indicates how quickly the transition takes place from low probability to high probability of a correct response.
> \[...\] A highly discriminating item divides the regions clearly-having a narrow region of ambiguity, that is, a region where the probabilities are intermediate in magnitude.

<!-- # TODO: Superficies de ítems con alta y baja discriminación -->

::: notes
Esta característica viene representada por el parámetro de "discriminación", que es una medida de la "calidad general" de un ítem.
Los ítems "buenos" serán muy discriminativos, lo que indica que distinguen bien entre la gente que tiene nivel de rasgo alto o bajo, mientras que los "malos" tendrán una superficie de respuesta más "plana".

La discriminación se puede entender con una especie de "dardo" que atraviesa de arriba abajo esa superficie perpendicularmente por esa "recta de inflexión" de la que hablábamos antes, y luego cruza el plano origen.
Si la superficie es "totalmente plana" el dardo entra por un punto y sale por otro que está justo debajo, en las mismas coordenadas de rasgo latente.
La "proyección" (si ponemos un foco justo encima, la sombra que forma en el plano) es un punto.
Es decir, la "discriminación" es cero (el vector del ítem tiene longitud "cero").
Si la pendiente es muy pronunciada (distingue bien entre niveles de rasgo) el dardo atravesará el plano origen muy lejos.
La sombra será muy larga (es decir, la discriminación muy alta).
El caso "extremo" es una superficie "vertical" en la recta de inflexión (un escalón; cosa imposible en ítems reales).
En este caso, el dardo nunca cortaría al plano origen, y la discriminación sería infinita.
:::

## Parámetros multidimensionales

@reckase_discriminating_1991:

<!-- # TODO: Fórmula de MDISC -->

<!-- # TODO: Gráfico del cálculo de MDISC como teorema de Pitágoras -->

<!-- # TODO: Fórmula de dificultad con MDISC -->

::: notes
@reckase_discriminating_1991 hacen la deducción de la discriminación multidimensional, y resulta dar lugar a esta fórmula.
Como veis es simplemente la "longitud" del vector de parámetros de discriminación, resultante simplemente de aplicar el teorema de Pitágoras.
El vector es un triángulo rectángulo, $a_1$ sería un cateto, el otro cateto sería igual a $a_2$, y la hipotenusa es la longitud (las dos discriminaciones al cuadrado, sumadas, y luego la raíz cuadrada de eso).
Esto por cierto también vale para cualquier número de dimensiones.
Aquí estamos representando sólo 2 porque es más fácil, pero podemos calcular la suma de cuadrados estos parámetros de discriminación (sea cual sea su número), y calcular su raíz cuadrada, y eso nos daría igualmente la discriminación multidimensional (la longitud de ese vector de $a$ del ítem).

Esto es en realidad el producto escalar (o producto punto) de un vector consigo mismo, que consiste en multiplicar una a una las componentes y sumarlos todos.
En forma matricial se representaría como el vector pre-multiplicado por sí mismo.
<!-- # TODO: Explicar la multiplicación vectorial (y poner ejemplo?) -->

Esta expresión de la discriminación multidimensional se puede usar para expresar de manera más sencilla la dificultad.
:::

## Representación gráfica

@ackerman_graphical_1996:

::: notes
La propuesta de representación gráfica completa (y consistente) no llega hasta 1996.
Ackerman propone que el ítem se represente como un vector de longitud dada por la discriminación multidimensional, aplicado en la posición del ítem (y en la dirección dada por esa posición; es decir, todos los ítems "apuntan" siempre en la dirección que pasa por el origen).
:::

## Supuestos: Monotonía creciente

> the probability of answering an item correctly increases monotonically with an increase in each dimension being measured

---@reckase_difficulty_1985 [p. 402]

<!-- # TODO: Ecuación modelo y ejemplo probabilidad constante -->

::: notes
Ahora bien, como hemos visto, la TRI originalmente sólo trataba con "ítems cognitivos" (lo hemos podido ver en el propio lenguaje que utiliza Reckase: "dificultad", "habilidad", "respuesta correcta").
Uno de los supuestos que hace es este.
Eso tiene todo el sentido (casi), ya que si la probabilidad de un ítem "decrece" al aumentar el nivel de habilidad, es que algo está mal en el ítem: No puede ser que alguien que sepa más tenga menos probabilidad de acertar un ítem que alguien que sepa menos.
Pero cuidado, si el ítem es "puro" de una dimensión, la probabilidad en la otra u otras dimensiones sería constante (discriminación = 0); es decir, la probabilidad puede ser "plana" (es una dimensión que no "está siendo medida por el ítem").
:::

## Generalización: Ítems inversos

<!-- # TODO: Curva de ítem inverso y ejemplo -->

<!-- # TODO: Fórmulas de los parámetros -->

<!-- # TODO: Ejemplo con a negativa -->

::: notes
Pero además, si hablamos de ítems "no cognitivos" (p.ej. personalidad) en los que la persona evaluada puede estar "de acuerdo" o "en desacuerdo", sí podemos tener ítems con probabilidad "decreciente".

La generalización a ítems inversos es directa: La discriminación multidimensional "eleva al cuadrado" los "parámetros de discriminación marginales", luego va a ser siempre positiva.
El parámetro de posición, por tanto, siempre va a tener el signo inverso al que tenga la intersección.
Los cosenos tendrán, cada uno, el mismo que el parámetro de discriminación de esa dimensión.
:::

## Generalización: Ítems inversos

<!-- # TODO: Representación de ítems con a (una o varias) negativa (tabla y gráfico) -->

::: notes
La representación de estos ítems sería como se muestra en este gráfico.
<!-- # TODO: Descripción de los ítems representados -->
:::

## Ítems "inversos" en la literatura

> the negative relation of Item 15 with geometry achievement is particularly puzzling

---@mcdonald_basis_2000 [p. 109]

> This result occurs because $a_1$ for the item is negative, and thus as an examinee's $\theta_1$ ability increases, the chance of answering the item correctly actually decreases.

---@ackerman_multidimensional_2005-1 [p. 17]

### Generalización: "Monotonía constante"

::: notes
Esta generalización es bastante evidente, y se puede aplicar sin problema a ítems no cognitivos.
Sin embargo, hay que tener en cuenta que Reckase explicitaba como condición que los ítems tenían que ser "monotonamente crecientes".
Pero en la literatura nos encontramos casos como estos, que no lo son, y a los autores no se les ocurre decir que "hay que generalizar esas definiciones".
Más bien, hacen comentarios como estos, restándole importancia al hecho de que los parámetros de discriminación sean negativos ("particularmente misterioso"), o afirmando lo obvio (el parámetro de discriminación del ítem es negativo para una dimensión, por lo que la probabilidad de respuesta correcta disminuye al incrementarse su habilidad)

En lugar de eso, proponemos que los supuestos de Reckase se "relajen" para considerar ítems con probabilidad "con monotonía constante" (es decir, que no tengan un "pico" u óptimo de rasgo latente para dar una respuesta positiva y luego decrezca), sino que cuanto más extremo sea el nivel de rasgo, más extrema es también la probabilidad (positiva o negativa).
Este caso general incluye también la probabilidad constante (ya que es monotonía "nula constante", es decir, "el cambio es nulo de manera constante").
Este tipo de ítems, positivos, negativos, o nulos, se pueden acomodar y modelar fácilmente por un modelo como el logístico de 2 parámetros multidimensional, como hemos visto.
Ítems que estarían excluidos, por no tener monotonía constante, podrían ser formulaciones como, por ejemplo, "Soy tan extrovertido como la mayoría de mis amigos" (alguien puede decir, "No, soy más introvertido que todos ellos", respuesta negativa con rasgo bajo, o "Qué va, soy el más extrovertido con diferencia", respuesta negativa con rasgo alto).
:::

## Supuestos: Ortogonalidad

> before performing the differentiation, the constraint that <!-- # TODO: Sum of cosines^2 = 1 --> is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

<!-- # TODO: Ejemplo de rotación oblicua con los ejes resultantes oblicuos -->

<!-- # TODO: Coordenadas rectangulares resultantes de una rotación oblicua -->

::: notes
Otro de los supuestos que hace Reckase originalmente para calcular los parámetros es este: Los cosenos al cuadrado suman 1.
Es decir, está aplicando el teorema de Pitágoras, pero ello implica de manera implícita que los ejes del espacio latente son "ortogonales" (forman ángulos de 90 grados).

Esto en TRI multidimensional originalmente tenía sentido, ya que no se tenía muy en cuenta la interpretación de los parámetros de rasgo latente.
O bien no importaba mucho que los ejes se pudieran interpretar como rasgos latentes, o se interpretaban de manera no necesariamente muy precisa.

No obstante, en 1999 llega McDonald, y dice que la TRI y el AF son fundamentalmente la misma cosa.
Pero en AF tenemos dimensiones correlacionadas la mayoría de las veces, y eso como hemos visto implica hacer rotaciones oblicuas que dan lugar a ejes no ortogonales.

Los pesos factoriales resultantes (o discriminaciones) los podemos representar sin ningún problema en coordenadas rectangulares (las llamadas "coordenadas Cartesianas").
Pero algo parece fallar cuando generalizamos los parámetros multidimensionales de TRI que hemos mostrado antes a esos espacios de coordenadas no ortogonales.
Si rotamos los ejes de coordenadas, pero los ítems siguen siendo los mismos, es de esperar que la "longitud" del vector de ese ítem (i.e. su discriminación), no cambie (esta propiedad se llama invarianza).
Veámoslo con un ejemplo.
:::

## Supuestos: Ortogonalidad

<!-- # TODO: Ejemplo de item en coordenadas ortogonales (gráfico y vectorial) -->

<!-- # TODO: Ejemplo de rotación oblicua (gráfico de ejes y matriz de rotación) -->

::: notes
Supongamos este ítem, en coordenadas ortogonales.
El vector muestra sus parámetros de discriminación en las dos dimensiones de los ejes representados.
Esto sería la "solución sin rotar" que resulta típicamente de ajustar un modelo de TRI multdimensional, o un AFE exploratorio.
La discriminación multidimensional de este vector es (explicar paso a paso).

Supongamos que aplicamos una rotación oblicua.
Hemos identificado dos rasgos latentes sustantivos, correlacionados entre sí.
Los ejes resultantes son los que se muestran en este gráfico.
Los nuevos parámetros de discriminación del ítem están representadas cada una por la "proyección" de ese vector sobre cada uno de los ejes (paralela al otro, u otros, si hay más de dos).
:::

## Supuestos: Ortogonalidad

<!-- # TODO: Ejemplo de item en coordenadas ortogonales (gráfico y vectorial) -->

<!-- # TODO: Ejemplo de item proyectado en los ejes rotados (gráfico y vectorial) -->

<!-- # TODO: Cálculo de la discriminación en los ejes rotados -->

::: notes
Hacer una rotación supone "multiplicar" las coordenadas de este vector por la matriz de rotación que representa ese nuevo sistema de coordenadas (la transformación del ortogonal a este nuevo, oblicuo).
En este caso, esta es la matriza de rotación.
Como los vectores se representan en columnas, se "pre-multiplica" por la matriz de rotación.
Esto es parecido a multiplicar por un vector; es como si cada fila de la matriz fuese un vector, y los resultados se ponen cada uno en una fila.
<!-- # TODO: Explicar paso a paso el producto matricial -->

Si ahora calculamos la discriminación del ítem en estos ejes rotados, nos encontramos con este resultado.
Es decir, ¿que el ítem discrimina mejor o peor en función de qué ejes quiera yo considerar?
Eso no parece tener sentido, y sin embargo hay muchos ejemplos en la literatura que calculan esta discriminación multidimensional con la fórmula de Reckase (con "rasgos correlacionados") sin preocuparse por "los ejes" que representan esos rasgos de manera fidedigna.
<!-- # TODO: Ejemplos de la literatura -->
:::

### Generalización: "Coordenadas oblicuas"

::: notes
Para la representación gráfica esto puede valer, como dice Ackerman aquí.
Pero a la hora de calcular los parámetros (y Ackerman lo hace en este capítulo de libro), solamente considerar el caso ortogonal, la información que obtenemos resulta deficiente.
:::

## Generalización: Coordenadas oblicuas

> before performing the differentiation, the constraint that <!-- # TODO: Sum of cosines^2 = 1 --> is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

<!-- # TODO: Expresión de las \thetas transformadas -->

<!-- # TODO: Distribución de \thetas en ejes ortogonales -->

::: notes
Hacer esta generalización no es algo tan directo como ocurría con la probabilidad "monótonamente decreciente".
Si os acordáis, Reckase aplicaba esta restricción de que la suma de cosenos directores era 1 (que se deriva del teorema de Pitágoras).
Pero para aplicarla hace falta asumir que esos cosenos directores son todos ortogonales entre sí.
Si no tenemos ángulos rectos, esa condición no se cumple.

Y lo que nosotros estamos asumiendo que tenemos una distribución de vectores de rasgo latente ($\theta$) con una matriz de correlaciones ($R$, la que sea).
Normalmente representamos esos vectores en coordenadas rectangulares (ejes ortogonales), pero para obtener los parámetros multidimensionales generalizados a coordenadas oblicuas tenemos que representados en un espacio no ortogonal (aquí también hablaría de "unidades no estandarizadas", es decir, que la distancia entre el o y el 1 pueda ser distinta de la base estándar, pero digamos solamente "no ortogonal" por simplificar).

¿Cómo podríamos solucionar esto?
Lo que podemos hacer es aplicar una "transformación" a los rasgos latentes de forma que pasemos de este espacio oblicuo (el que sea, aún no sabemos cuál es) a un espacio ortogonal.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Expresión de las \thetas transformadas -->

<!-- # TODO: Expresión de la R transformada -->

<!-- # TODO: Distribución (esférica) de las \thetas transformadas -->

::: notes
Esto es una transformación de las coordenadas, como la que aplicamos antes para calcular las coordenadas del ítem tras aplicar la rotación oblicua, pero aplicada en este caso al vector de rasgo latente.

Estas nuevas coordenadas (transformadas), que ahora están en un espacio ortogonal, queremos que representen una distribución en la que las correlaciones sean nulas, es decir, que la matriz de correlaciones transformada ($R^B$) sea una matriz identidad (unos en la diagonal, ceros en el resto).
Pero hay que tener en cuenta que esto es un "artefacto matemático".
Estamos haciendo esto por conveniencia, pero las coordenadas que realmente nos interesan como investigadores, porque son las que verdaderamente representan algo que nos interesa investigar, son esas coordenadas originales, que pueden representar cualquier fenómeno que sea de nuestro interés (rasgos de personalidad, síntomas, actitudes, inteligencia, conocimientos... lo que estemos investigando).
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Ejemplo de \theta (expresión) y coordenadas en espacio oblicuo -->

::: notes
Mientras tanto, las coordenadas de los "rasgos latentes" originales se van a representar en otros ejes, en general distintos de los "ejes ortogonales" transformados.
Esto lo que significa es que las puntuaciones de rasgo latente originales van a representar la posición de un participante respecto de esos ejes originales.
Por ejemplo, en este caso, si el participante tiene un "vector de puntuaciones" \[1, 2\], para representar sus coordenadas tenemos que desplazarnos, primero una unidad en el primer eje, y luego dos unidades en el segundo eje.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Opciones de elección de bases oblicuas: rectangular, ejes muy alineados, ejes correctos -->

::: notes
En base a lo planteado anteriormente, me gustaría hacer una prueba con vosotras y vosotros.
Supongamos una distribución bivariada de rasgos latentes, con una correlación de .8, bastante alta. ¿Cuál de los siguientes sistemas de coordenadas consideráis que representaría adecuadamente los ejes de una distribución bivariada, de forma que al transformarlos a coordenadas rectangulares obtuviésemos una distribución con correlación de 0?
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Distribución en ejes rectangulares -->

::: notes
El primero ya sabemos que no, de acuerdo, puesto que es la representación habitual, y sabemos que la nube que forma está "achatada".
No se aplica ninguna transformación (la transformación es una matriz identidad), ya que los que serían los ejes originales y los transformados son los mismos.
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Animación de distribución en ejes rectangulares a ejes alineados -->

::: notes
Si pensáis igual que yo, habréis elegido el segundo, porque el coseno del ángulo entre los ejes representa la correlación entre las dimensiones.
Por lo tanto, ejes muy alineados representan una correlación muy alta.

Sin embargo, fijáos lo que ocurre cuando "transformamos" las coordenadas de los puntos de nuestra distribución a ese sistema de coordenadas.
Como podéis ver, la distribución tiende a "achatarse" aún más, en lugar de hacerse más "esférica".
:::

## Generalización: Coordenadas oblicuas

<!-- # TODO: Matriz de correlaciones con correlación muy alta -->

<!-- # TODO: Animación de distribución en ejes rectangulares a ejes correctos -->

<!-- # TODO: Matriz de correlaciones transformada "identidad" -->

::: notes
El sistema de coordenadas correcto es el tercero, que "compensa" de alguna manera el achatamiento de la nube de puntos, de forma que las coordenadas transformadas que nos quedan (en los ejes ortogonales), los "normales" del gráfico) tienen una distribución "esférica" (con matriz de correlaciones "identidad").

Esto, que visto así parece ya evidente, es algo que al menos a mí me ha llevado varias decenas (puede que "cientos") de horas entender, durante al menos dos años trabajando en este problema.
:::

## Espacio latente vs. espacio test

> It has been suggested by some researchers that the angle between the axes represents the degree of correlation.
> However, for sake of clarity, an orthogonal axes system is used in which distance measures and vectors can be easily calculated, understood, and interpreted.

---@ackerman_multidimensional_2005-1 [p. 16]

<!-- # TODO: Fórmula del modelo en coordenadas "transformadas" y equivalencia -->

<!-- # TODO: Expresión de la transformación de la discriminación a coordenadas rectangulares -->

::: notes
Viendo que en la literatura hay afirmaciones como esta, no me resulta totalmente descorazonador.
Parece que no soy el único al que le ha supuesto un esfuerzo entenderlo (resultados de la "encuesta"?)

Lo que ocurre en realidad, y esto es en realidad también un paralelismo con lo que ocurre en análisis factorial, es que para que el modelo sea invariante, los ítems también tienen que transformarse a un sistema de coordenadas ortogonal.
Pero podemos ver que su matriz de transformación no es la misma que la de las puntuaciones de rasgo latente.
Es su inversa (inversa traspuesta, más concretamente).

Y esto nos ha permitido "descubrir" (o darnos cuenta, más bien) de que hay dos "espacios vectoriales" distintos, uno en el que se encuentran las coordenadas de las puntuaciones factoriales, y otro distinto, que es el espacio en el que se encuentran las coordenadas de los ítems, y que es donde en realidad hay que representarlos.
:::

## Espacio latente vs. espacio test

> \[...\] the latent space should be regarded as a general Euclidean space with its *inner product* defined by <!-- # TODO: Inner product -->.

---@zhang_theoretical_1999 [p. 221]

<!-- #TODO: Ref. to Mahalanobis -->

> \[...\] the *inner product* in \[the test space\] is consistently defined by <!-- # TODO: Inner product -->.

---@zhang_theoretical_1999 [p. 221]

::: notes
Esto, curiosamente, aparece en la literatura como podéis ver; en el año 1999, en un artículo que no tiene nada que ver con esto.
Lo menciona de soslayo y no profundiza en la deducción de los parámetros, como hemos hecho aquí.S olamente lo utiliza para resolver un problema "parcial" que se encuentra en la deducción de un índice de dimensionalidad, distinto de los parámetros de los que estamos hablando aquí.
Pero podía haber obviado este problema y haber utilizado coordenadas ortogonales, sin importarle si había o no correlaciones entre los rasgos latentes, como ha hecho todo el mundo.

Así que, todos mis respetos por estos autores.
Sin embargo claro, ellos estaban hablando de otra cosa, y sólo tocaban este tema de paso.
Así que al final eso se quedó ahí, y nadie ha hecho caso en 25 años a lo que dijeron Zhang y Stout.
La gente ha usado los modelos, hecho simulaciones, etc., usando distribuciones correlacionadas, y ha usado las fórmulas de Reckase para transformar entre parámetros multidimensionales y los parámetros "lineales" (podríamos llamarlos, los de la formulación original) del modelo.
:::

## Parámetros multidimensionales generalizados

<!-- # TODO: Fórmulas de los parámetros multidimensionales -->

::: notes
Si tenemos en cuenta las correlaciones (más generalmente, covarianzas) de la distribución de puntuaciones factoriales, lo que obtenemos son estas expresiones para los parámetros multidimensionales.

Muy importante, la distancia con signo y la discriminación multidimensional son iguales, no importa en qué espacio queramos obtenerlas y representarlas.
Pero los cosenos directores es necesario calcularlos en el espacio test, no en el espacio de puntuaciones latentes, como decía Reckase, porque este es el espacio que va a representar en realidad la relación entre las variables latentes.
:::

## Conclusiones

Parámetros multidimensionales generalizados

-   A ítems con monotonía constante

-   A coordenadas oblicuas

Distinción espacio latente - espacio test

::: notes
En conclusión, lo que hemos propuesto en este artículo consiste en una generalización:

-   A ítems con monotonía constante, creciente, decreciente o nula.
    Esto se venía haciendo ya como hemos visto, pero creemos que hacía falta formalizarlo.
    No basta con hacerlo y punto, si Reckase dijo que las discriminaciones tiene que ser positivas, tenemos que demostrar que también vale para discriminaciones negativas o nulas (monotonía decreciente, o probabilidad constante).

-   A coordenadas oblicuas, las cuales consideramos que son necesarias para representar distribuciones de puntuaciones de rasgo correlacionadas, en general.
    Hemos visto que esto ya lo mencionaron Zhang y Stout, hace 25 años, pero no proporcionaron una "deducción" de estos parámetros multidimensionales, cosa que nosotros hacemos.

Por último, es imortante recalcar este "descubrimiento" (o "redescubrimiento", más bien) del espacio test, ya que tenemos que ser muy conscientes de que los ítems y las personas están en espacios vectoriales distintos.
Aunque esto parezca solamente un "artefacto matemático" (y en el fondo lo es), hemos visto que es verdaderamente crucial tenerlo en cuenta para entender bien los modelos que estamos utilizando.
Además, esta "distinción" se halla también en los modelos factoriales: Existe lo que se denominan "ejes primarios" y "ejes de referencia", propuestos por Thurstone hace años (pero que prácticamente no se utilizan ni tienen en consideración).
<!-- # TODO: Hace cuántos años? (Thurstone) --> Sin embargo, cuidado con esto, porque no está claro que esos espacios en TRI multidimensional se puedan asimilar directamente a los ejes en un modelo de AF.
:::

## To be continued...

-   Paralelismo TRI multidimensional - AF

-   Extensiones del modelo M2PL: Escala graduada, respuesta nominal, 3PL...

-   Investigación en otros modelos

::: notes
Esta de hecho puede ser una futura línea de investigación.
Puede parecer evidente el paralelismo (o al menos a mí me lo parece), pero si una cosa me ha enseñado el estar trabajando en este problema de los parámetros (y toparme con el problema de los dos espacios), es que no hay que fiarse de las apariencias.
Por eso aplicamos el método científico (el método puramente deductivo, en este caso)

El modelo multidimensional logístico de 2 parámetros se puede considerar como "anidado", o un caso más simple de varios modelos.
Así que partiendo de este como base, lo interesante es aplicarlo a otros más complejos, pero más útiles en la práctica.
Por cierto, que esto también lo hace mucha gente, aplica las fórmulas de Reckase a otros modelos y a correr, se quedan "tan anchos".

El modelo de escala graduada, por ejemplo, sería el equivalente al modelo de AF para respuestas ordinales, con varios umbrales o "posiciones" en TRI.

El de crédito parcial, por ejemplo, puede servir también para formatos con varias opciones de respuesta que miden o pesan en factores diferentes cada una de ella.

El logístico de 3 parámetros es como el de 2, pero añadiendo un parámetro de pseudoazar, para modelar el que un participante acierte un ítem por pura suerte, respondiendo "a ciegas" (esto sólo valdría para ítems cognitivos).

Estos modelos, que son "extensiones" de este más básico, deberían partir de estos parámetros como fundamento.
Pero un investigador no debería dar los parámetros sin más de ese modelo, sin proporcionar una prueba formal de que se aplica esa fórmula y se cumplen los supuestos.
:::

## Últimas palabras

<!-- # TODO: Imagen ilustrativa -->

::: notes
A este respecto, me gustaría terminar recalcando esto último que he mencionado, aplicado no sólo a cuestiones psicométricas, o de TRI.
Cuando estaba empezando a preparar este seminario, una de mis inquietudes era la complejidad, tanto conceptual como formal de esta investigación.
Compartiendo estas inquietudes con algunos compañeros, me aconsejaron utilizar digamos este discurso como excusa para lanzar este "mensaje" en general: La idea de que tenemos que ser muy precavidos a la hora de aplicar estos métodos, sean cuales sean (cuantitativos, cualitativos, o del tipo que sean) sin reflexionar sobre si son los más adecuados o no.
Y bueno, a este respecto comentar que, no sólo yo, sino el departamento de Metodología al completo, hacemos una tipo de investigación que precisamente se encarga de eso, de intentar entender estos métodos, saber si son los más adecuados, y saber cuándo, cómo, y sobre todo, por qué, aplicarlos en la investigación aplicada (aplicada para nosotros, al menos).

Así que bueno, ya lo sabéis, estamos disponibles en el departamento para colaborar en esas investigaciones, si lo consideráis oportuno, para aplicar los métodos más apropiados a los problemas de investigación que tengáis.
En mi caso en concreto, no sólo puedo proporcionar cierto "expertise" psicométrico.
También me dedico mucho a cuestiones de ciencia abierta y reproduciblidad, así que si queréis darle a vuestra investigación un giro en ese sentido, para hacerla más sostenible y alineada con los objetivos de los organismos financiadores (y si no queréis, deberíais ir queriendo), podéis contactar conmigo, y podemos plantear líneas de colaboración.
Eso sí, me gustaría pediros, si necesitáis colaboración, no sólo metodológica, sino de cualquier tipo, por favor no nos llaméis cuando tengáis un lío de datos, y no sepáis bien cómo resolver la papeleta (no digo que sea vuestro caso, pero me he encontrado con muchos casos, que son verdaderas historias de terror).
Contad por favor con nosotros desde el momento de escribir vuestro proyecto, para solicitar la financiación, porque es en ese punto en el que se pueden prevér problemas y anticiparse a ellos.
Porque si una investigación no está bien diseñada desde el principio, teniendo en cuenta estos aspectos metodológicos, de reproducibilidad, etc., si tenemos mala suerte pueden surgir problemas que se puedan resolver sólo con muchísimo trabajo, o incluso que ya nunca se puedan resolver, y tengamos que tirar a la basura todo ese esfuerzo de investigación.

Eso es todo, muchas gracias.
Y espero vuestras preguntas y comentarios.
:::

## References {.smaller}
