---
title:     |
  A generalization of multidimensional item response theory parameters
author:    |
  <table>
    <tr>
      <td>Daniel Morillo, Ph.D.</td>
      <td>Mario Luzardo, Ph.D.</td>
    </tr>
    <tr>
      <td>
        <center>
          [![](../www/slideshow-assets/github-logo.png){height="50"}](https://github.com/DaniMori/)&emsp;
          [![](../www/slideshow-assets/orcid.png){height="50"}](https://orcid.org/0000-0003-3021-3878)
        </center>
      </td>
      <td>
        <center>
          [![](../www/slideshow-assets/github-logo.png){height="50"}](https://github.com/mluzardov/)&emsp;
          [![](../www/slideshow-assets/orcid.png){height="50"}](https://orcid.org/0000-0002-9360-2806)
        </center>
      </td>
    </tr>
  </table>
institute: |
  <table>
    <!-- # TODO: Fix vertical alignment in logos-->
    <tr>
      <td style="width: 400px">
        [![](../www/slideshow-assets/logos_fac_psicologia.svg){height="60"}](https://www.uned.es/universidad/facultades/psicologia.html)
      </td>
      <td style="width: 100px">
        [![](../www/slideshow-assets/logo_udelar.svg){height="120"}](https://udelar.edu.uy)
      </td>
    </tr>
  </table>
bibliography:  ../www/Multidimensional-parameters-MCLM.bib
csl:           ../www/apa-old-doi-prefix.csl
date:          "2024-07-17"
date-meta:     "2024-11-27"
date-format:   long
editor:
  mode:        source
  markdown: 
    wrap:      sentence # TODO: Restore
    canonical: true     # TODO: Restore
execute: 
  cache:       true
lang:          en
knitr:
  # opts_knit: # Does not work, see line 102
  #   root_dir:  here::here()
  opts_chunk: 
    results:   asis
    autodep:   true
format:
  revealjs:
    auto-stretch:            true
    code-annotations:        hover
    df-print:                paged
    fig-cap-location:        bottom
    fig-format:              svg
    fig-asp:                 1
    fig-width:               6
    incremental:             false
    keep-md:                 false
    link-external-newwindow: true
    margin:                  0
    self-contained:          true
    slide-number:            false
    theme:                   ../www/slideshow-assets/extra-styles.scss
    transition:              none
    view-distance:           3
    template-partials:
      - ../www/slideshow-assets/title-slide.html
editor_options:
  chunk_output_type: console
---

```{r libraries}
#| cache:   false
#| include: false

library(tibble)
library(dplyr)
library(knitr)
library(lavaan)
library(semPlot)
library(extrafont)
library(tidyr)
library(ggplot2)
library(units)
library(ggtext)
library(ggridges)
library(qgraph)
library(stringr)
library(purrr)
library(plotly)
library(metR)
library(scales)
library(fontawesome)
library(mvtnorm)
library(patchwork)
```

```{r setup}
#| cache:   false
#| include: false
opts_knit$set(root.dir = here::here()) # See lines 43-44

# Graphical configuration options:
loadfonts(device = "win") # Load fonts (if necessary)
par(family = "Arial", adj = 0.5) # Set graphical device params
```

```{r sources}
source("R/Formulae.R",                  encoding = 'UTF-8')
source("R/Output.R",                    encoding = 'UTF-8')
source("R/Mirt_toolbox.R",              encoding = 'UTF-8')
source("src/Graphical_example_paper.R", encoding = 'UTF-8')
```

```{r graphical-constants}
# Color palettes:

PALETTE_UNED <- c(
  normal = "#00533e",
  medium = "#427562",
  light  = "#86a699",
  white  = "#cddad5"
)
PALETTE_APPLE <- c(
  normal = "#749f4c",
  medium = "#8aac5d",
  light  = "#a2bc7e",
  white  = "#cddbb8"
)
PALETTE_BLUE <- c(
  normal = "#5c6eb1",
  medium = "#7683bd",
  light  = "#929acb",
  white  = "#d4d6eb"
)
PALETTE_TANGERINE <- c(
  normal = "#d76f47",
  medium = "#dd8964",
  light  = "#e4a482",
  white  = "#f0cfb9"
)
PALETTE_STRAWBERRY <- c(
  normal = "#da5268",
  medium = "#c87384",
  light  = "#daa6b1",
  white  = "#efdbdf"
)
PALETTE_RASPBERRY <- c(
  normal = "#90214a",
  medium = "#8e4d60",
  light  = "#af848b",
  white  = "#dac7c7"
)

# Factor axis colors:
F1_COLOR <- unname(PALETTE_BLUE["normal"])
F2_COLOR <- unname(PALETTE_RASPBERRY["normal"])

# Item colors:
ITEMS_PALETTE <- c(
  PALETTE_BLUE["normal"],
  PALETTE_RASPBERRY["normal"],
  PALETTE_APPLE["normal"],
  PALETTE_TANGERINE["normal"],
  PALETTE_UNED["medium"]
) |>
  unname() |>
  setNames(1:5)

# Grid breaks:
X_BREAKS <- seq(-3, 3, length.out = 5)

# Probability axes:
PROB_AXIS <- seq(-3.3, 3.3, by = .01)
PROB_AXIS_2D <- seq(-3, 3, by = 0.05) # Axis for the probability contour plots
```

```{r graphical-output-conf}
#| cache: false

# Graphical output constants:
GRAPH_FONT   <- "Arial"  # Replace default font to comply with UNED identity
FONT_SIZE    <- 24       # Replace default font size for slideshow
AXIS_LAB_POS <-  0.5     # Replace default axis label position for slideshow
LINE_WIDTH   <-  0.75    # Replace default line width for slideshow
GRID_WIDTH   <-  0.5     # Width for grid and axis lines
AXIS_COLOR   <- "gray70" # Replace default grid line color for slideshow
VECTOR_WIDTH <- 1.5      # Replace default vector width for slideshow

theme_set( # `ggplot` output configuration
  theme_classic(
    base_size      = FONT_SIZE,
    base_family    = GRAPH_FONT,
    base_line_size = LINE_WIDTH
  ) %+replace%
    theme(
      axis.title.x       = element_text(hjust = AXIS_LAB_POS),
      axis.title.y       = element_text(vjust = AXIS_LAB_POS, angle = 0),
      axis.title.y.right = element_text(vjust = AXIS_LAB_POS, angle = 0),
      panel.grid         = element_line(color = AXIS_COLOR),
      panel.grid.minor   = element_line(linetype = "17", color = AXIS_COLOR)
    )
)

MARGINS <- c(12, 1.3, 8, 1.3) # margins for the sem path plots
```

## Unidimensional IRT {.smaller}

-   2-parameter logistic (2PL) model [@birnbaum_latent_1968]:

::::::::: columns
::::::: {.column width="40%"}
$$
  P(Y_i = 1) = \Phi_L (a_i (\theta - b_i))
$$

$$
  \Phi_L (x) =  \frac{1}{1 + e^{-x}}
$$

<br>

:::::: {.fragment fragment-index="1"}
::: r-stack
[$a_1$ = 1.0; $b_1$ = 0.0]{style="color:#5c6eb1;"}
:::

::: r-stack
[$a_2$ = 1.0; $b_2$ = 1.5]{style="color:#90214a;"}
:::

::: r-stack
[$a_3$ = 2.5; $b_3$ = 0.0]{style="color:#749f4c;"}
:::
::::::
:::::::

::: {.column .fragment width="60%" fragment-index="1"}
```{r irt-curves}
#| fig-asp:   0.6
#| fig-width: 8

ITEMS <- tribble(
  ~item, ~a,   ~b,
   1,     1,    0,
   3,     1,    1.5,
   2,     2.5,  0
)


logit_curve_data <- tibble(
  latent_trait = PROB_AXIS,
  prob         = ITEMS |>
    select(-item) |>
    pmap(irf, theta = latent_trait) |>
    bind_cols()
) |>
  unnest(
    prob,
    names_sep = '_'
  ) |>
  rename_with(str_remove, pattern = '\\.{3}')

logit_curve_data_long <- logit_curve_data |>
  pivot_longer(starts_with("prob"), names_to = "item", values_to = "prob") |>
  mutate(item = item |> str_remove("prob_"))

logit_curve_data_long |>
  ggplot(
    mapping = aes(
      latent_trait, prob,
      color = item,
      group = after_scale(color)
    )
  )                                        +
  geom_line(linewidth = LINE_WIDTH)        +
  labs(x = "*&theta;*", y = "P(Y~i~ = 1)") +
  scale_y_continuous(
    breaks       = 0:1,
    minor_breaks = NULL,
    limits       = 0:1,
    expand       = expansion(mult = c(0, .05))
  )                                        +
  scale_x_continuous(
    breaks       = X_BREAKS,
    minor_breaks = NULL,
    limits = range(PROB_AXIS),
    expand = expansion()
  )                                        +
  scale_color_manual(values = ITEMS_PALETTE, guide = NULL) +
  theme(
    axis.title       = element_markdown(),
    axis.title.y     = element_markdown(angle = 90),
    panel.grid.major = element_line(linetype = "17", linewidth = GRID_WIDTH),
  )
```

<!-- TODO: Como ecuaciones (sin punto y coma?) -->
:::
:::::::::

::: {.r-stack .fragment fragment-index="1"}
$a_i$ = discrimination
:::

::: {.r-stack .fragment fragment-index="1"}
$b_i$ = difficulty (location)
:::

::: notes
The 2-parameter logistic model is the most simple model in item response theory (IRT).

It is a prototypical model that allows us to study the item properties according to a parametric conception...

\[NEXT\]

... that assumes a discrimination ($a_i$) and a difficulty (or location, $b_i$) for each item.
:::

## Multidimensional IRT {.smaller}

-   Multidimensional 2PL (M2PL) model [@mckinley_extension_1983]:

:::::: columns
:::: column
$$
  P(Y_i = 1) = \Phi_L (\mathbf{a}_i^T \mathbf{\theta} + d_i)
$$

::: {.fragment fragment-index="1"}
```{r mirt-model-irs-3d}
#| fig-width:  5
#| fig-height: 5
mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,     1,    0.5,  -0.5
)

irs2d <- function(a_1, a_2, d, theta_1, theta_2) {
  
  logit(a_1 * theta_1 + a_2 * theta_2 + d)
}

mirt_item_probs <- expand_grid(
  trait_2 = PROB_AXIS_2D,
  trait_1 = PROB_AXIS_2D
) |>
  mutate(
    mirt_item |>
      select(-item) |>
      pmap(irs2d, theta_1 = trait_1, theta_2 = trait_2) |>
      bind_cols()
  ) |>
    rename(prob = `...1`)

mirt_prob_surface <- mirt_item_probs |>
  pivot_wider(names_from = trait_1, values_from = prob) |>
    select(-trait_2) |>
    as.matrix()

plot_ly(
  x = ~PROB_AXIS_2D,
  y = ~PROB_AXIS_2D,
  z = ~mirt_prob_surface,
  colorscale = list(c(0, 1), PALETTE_BLUE["normal"] |> rep(2)),
  type = 'surface',
  hovertemplate = 'P(I<sub>i</sub> = 1) = %{z:.2f}<extra></extra>',
  showscale = FALSE,
  hoverinfo = 'Z'
) |>
  layout(
    scene = list(
      xaxis = list(title = "<i>θ</i><sub>1</sub>"),
      yaxis = list(title = "<i>θ</i><sub>2</sub>"),
      zaxis = list(title = "P(I<sub>i</sub> = 1)"),
      camera = list(
        center = list(x = -.1, z = -.2),
        eye = list(x = -.12, y = -1.8, z = 1)
      )
    )
  ) |>
  config(displayModeBar = FALSE, scrollZoom = FALSE)
```
:::
::::

::: {.column .fragment fragment-index="1"}
<br>

```{r mirt-model-irs-contour}
contour_plot <- mirt_item_probs |>
  ggplot(mapping = aes(trait_1, trait_2, z = prob, label = after_stat(level))) +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_contour2(
    color = F1_COLOR,
    linewidth = LINE_WIDTH,
    breaks = c(.05, .1, .2, .3, .5, .7, .8, .9, .95),
    label.placer = label_placer_fraction(frac = .90, rot_adjuster = 0)
  ) +
  scale_x_continuous(
    minor_breaks = NULL,
    limits = range(PROB_AXIS_2D),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    minor_breaks = NULL,
    limits = range(PROB_AXIS_2D),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  coord_fixed() +
  theme(
    axis.ticks       = element_blank(),
    axis.line        = element_blank(),
    axis.title       = element_markdown(),
    axis.title.y     = element_markdown(),
    panel.grid.major = element_line(linetype = "17", linewidth = GRID_WIDTH)
  )

contour_plot
```
:::
::::::

::: notes
The Multidimensional 2PL model is one of many extensions of the 2PL model, which in this case is compensatory.
It assumes that the probability of a correct response to an item is a function of the linear combination of the latent traits weighted by item discrimination parameters.

\[NEXT\]

This results in a response probability function where equal probability points form a straight line in the latent space.
:::

## Multidimensional parameters {.smaller}

<br>

-   Unidimensional 2PL model: $P(Y_i = 1) = \Phi_L (a_i (\theta - b_i))$

    -   $a_i$ = discrimination

    -   $b_i$ = difficulty (location)

-   Multidimensional 2PL model: $P(Y_i = 1) = \Phi_L (\mathbf{a}_i^T \mathbf{\theta} + d_i)$

    -   $\mathbf{a}_i$ = ?

    -   $d_i$ = ?

::: fragment
@reckase_difficulty_1985 [p. 402]:

> \[...\] means of describing the characteristics of an item that takes into account the dimensionality of the skills \[...\] can then be used to determine how or if it is possible to compare items that measure different combinations of abilities.
:::

::: notes
The problem with the M2PL, and other multidimensional extensions, is that its parameters do not have a clear interpretation in IRT terms, and they cannot be reparameterized.

\[NEXT\]

That is why Reckase proposes to find a way of describing the characteristics of multidimensional items, taking their dimensionality into account.

He do it in this 1985 paper, and in a subsequent one in 1991...
:::

## Multidimensional parameters {.smaller}

<br>

::::: columns
::: column
-   @reckase_discriminating_1991:

$$
  MDISC_i = \sqrt{\mathbf{a}_i^T \mathbf{a}_i}
$$

<br>

-   @reckase_difficulty_1985:

$$
  MID_i = \begin{Bmatrix}
    D_i &= \frac{-d_i}{MDISC_i} \\
    \cos \mathbf{\alpha}_i &= \frac{\mathbf{a}_i}{MDISC_i}
  \end{Bmatrix}
$$
:::

::: {.column .fragment}
-   @ackerman_graphical_1996:

```{r m2pl-representation}
#| fig-align:  center
#| fig-height: 6.2
#| fig-width:  6.2

mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,     1,    0.5,  -0.5
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
    rename_with(str_remove, pattern = "transf_")

d_i_spike <- mirt_item_coords |>
  select(item, trait_1 = origin_1, trait_2 = origin_2) |>
  bind_rows(tibble(item = 1, trait_1 = 0, trait_2 = 0)) |>
  add_column(type = c("end", "start")) |>
  pivot_wider(names_from = type, values_from = starts_with("trait"))

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    data    = d_i_spike,
    mapping = aes(
      trait_1_start, trait_2_start,
      xend = trait_1_end,
      yend = trait_2_end
    ),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43',
    color = PALETTE_BLUE["light"]
  ) +
  geom_point(
    data = mirt_item_coords,
    mapping = aes(origin_1, origin_2),
    inherit.aes = FALSE,
    size = I(4)
  ) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-1, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-1, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = unname(PALETTE_BLUE["normal"]), guide = NULL) +
  annotate(
    geom = "richtext",
    x = 0.16, y = 0.20,
    label = "*D~i~*",
    size = 9,
    angle = mirt_item_params |> pull(deg_1),
    label.colour  = NA,
    fill = NA
  ) +
  annotate(
    "richtext",
    x = mirt_item_coords |> pull(origin_1) + 0.40,
    y = mirt_item_coords |> pull(origin_2) + 0.32,
    label = "*MDISC~i~*",
    size  = 9,
    angle = mirt_item_params |> pull(deg_1),
    label.colour  = NA,
    fill = NA
  ) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title         = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = .4),
    axis.title.y.right = element_markdown(vjust = .4),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    ),
  )
```
:::
:::::

::: notes
...Reckase & McKinley define the item discrimination, which is an extension of the Pythagorean theorem.

Note also that the discrimination can in turn be used to define the item difficulty, as happens in the unidimensional model.

And also, that the difficulty has two components: a "signed distance" from the origin, and a direction.

\[NEXT\]

It is easier to appreciate this with the graphical representation, that was proposed by Ackerman a few years later, where we can see that the item vector is applied at a point given by the difficulty, and has a length equal to the discrimination.
Note that the item direction always goes through the origin, and that the sign of the distance (positive in this case) will determine whether the vector is displaced forward or backward along that direction.
:::

## Assumptions {.smaller}

<br>

@reckase_difficulty_1985:

-   Increasing monotony:

> the probability of answering an item correctly increases monotonically with an increase in each dimension being measured

---(p. 402)

-   Orthogonality

> \[...\] before performing the differentiation, the constraint that Σ cos^2^**α** = 1 is added to the expression for the slope \[...\]

---(p. 404)

::: notes
However, there are two assumptions that Reckase & McKinley make when obtaining the definition of the multidimensional parameters.

The first is that the probability of answering an item correctly always increases monotonically with respect to each dimension.

The second, that constrain that you see there, implies that the item is represented in an orthogonal basis, which is why the discrimination is defined by a "general Pythagorean theorem".

So, our purpose in this work is **to relax these two assumptions**.
:::

## Assumptions: Increasing monotony

<br>

::::: columns
::: column
<br>

<!-- TODO: Same color as item? -->

$$
  \begin{align}
    \mathbf{a}_1 &=
      \begin{bmatrix}
        1.5 \\
        0
      \end{bmatrix} \\
      \\
      \\
    d_1 &= 0.75
  \end{align}
$$
:::

::: column
```{r m2pl-representation-unidimensional}
#| fig-align:  center
#| fig-height: 7.2
#| fig-width:  7.2

mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,     1.5,  0,    0.75
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
    rename_with(str_remove, pattern = "transf_")

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-1, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-1, 1),
    breaks       = NULL,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = unname(PALETTE_BLUE["normal"]), guide = NULL) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title         = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = .4),
    axis.title.y.right = element_markdown(vjust = .4),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    ),
  )
```
:::
:::::

::: notes
In the first case, one may argue that we may deal with "undimensional items" or, more generally, items that have null discrimination components.

This would imply that the probability of a correct response would not change with respect to that dimension; i.e., that trait is not being measured by the item, so the isoprobability lines would be parallel to the $\theta_2$ axis (in this example.

Therefore, the probability does not "increase monotonically".

Nevertheless, with this caveat about null discrimination components, this assumption would be realistic in a "cognitive testing context": The more you know, the more likely you are to give a correct answer.
:::

## Assumptions: Increasing monotony {.smaller}

<br>

> "*I neglect my duties*"

-- [International Personality Item Pool](https://ipip.ori.org/newAB5CKey.htm#Conscientiousness)

<br>

:::::: {.columns .fragment}
::: {.column .r-center}
```{r inverse-item-curve}
#| fig-align:  center
#| fig-asp:    0.5344
#| fig-width:  9

ITEMS <- tribble(
  ~item, ~a,   ~b,
   1,    -1.5, 0
)

logit_curve_data <- tibble(
  latent_trait = PROB_AXIS,
  prob         = ITEMS |>
    select(-item) |>
    pmap(irf, theta = latent_trait) |>
    bind_cols()
) |>
  unnest(
    prob,
    names_sep = '_'
  ) |>
  rename_with(str_remove, pattern = 'prob_\\.{3}')

logit_curve_data_long <- logit_curve_data |>
  pivot_longer(matches('\\d'), names_to = "item", values_to = "prob")

logit_curve_data_long |>
  ggplot(
    mapping = aes(
      latent_trait, prob,
      color = item,
      group = after_scale(color)
    )
  )                                                        +
  geom_line(linewidth = LINE_WIDTH)                        +
  labs(x = "Conscientiousness", y = "P(Y~i~ = 1)")           +
  scale_y_continuous(
    breaks       = 0:1,
    minor_breaks = NULL,
    limits       = 0:1,
    expand       = expansion(mult = c(0, .05))
  )                                                        +
  scale_x_continuous(
    minor_breaks = NULL,
    expand = expansion()
  )                                                        +
  scale_color_manual(values = ITEMS_PALETTE, guide = NULL) +
  theme(
    axis.title       = element_markdown(),
    axis.title.x     = element_text(margin = margin(t = 1, unit = "lines")),
    axis.title.y     = element_markdown(angle = 90),
    panel.grid.major = element_line(linetype = "17", linewidth = GRID_WIDTH),
    # The following is necessary to avoid showing the axis breaks, as dropping
    #   them prevents the axis line from being drawn (see
    #   https://github.com/tidyverse/ggplot2/issues/2983)
    axis.text.x.bottom = element_blank(),
    axis.ticks.length.x.bottom = unit(0, units = "cm"),
    panel.grid.major.x = element_blank()
  )
```
:::

:::: {.column .r-center}
$$
  P(Y_i = 1) = \Phi_L (a_i (\theta - b_i))
$$

<br>

::: r-stack
[$a_i < 0$]{style="color:#5c6eb1;"}
:::
::::
::::::

::: notes
However, if we wanted to apply this model to "non-cognitive" items (e.g., personality traits) where the person evaluated can be "agreeing" or "disagreeing", we can have items with "decreasing" probability.

\[NEXT\]

Formally, what this would imply is a negative discrimination parameter.
:::

## Generalization: Inverse items {.smaller}

::: notes
Si recordáis la "función característica del ítem", esto consiste simplemente en un valor negativo en el parámetro $a$; eso hace que la probabilidad sea decreciente.

Llevado al caso multidimensional, lo que nos podemos encontrar es con ítems que por ejemplo tengan todos los valores de discriminación negativos, o algunos negativos y otros positivos.
Pensad por ejemplo que tenemos el problema de los "sesgos de respuesta".
Vamos a imaginar que alguien estuviera respondiendo con un sesgo de "deseabilidad social" (intenta dar una buena imagen de sí mismo o de sí misma, de manera consciente o no).
A un ítem como este, tendería a responder que no, dado ese sesgo de deseabilidad social, al margen de cómo autoevalúe su verdadero nivel de responsabilidad.
Por lo tanto, tendríamos un ítem con dos valores negativos de discriminación, uno en la dimensión de responsabilidad, y otro en la dimensión de deseabilidad social.
Sin embargo, consideremos ahora que estamos modelando un sesgo de "aquiescencia", que hace que la gente tienda a estar de acuerdo con todo, o a decir que sí a todo sin un criterio muy reflexivo ("¿Descuidas tus obligaciones? Mmmmm... sí"; "¿Atiendes tus obligaciones con diligencia? Mmmmm... sí").
Cuanto más aquiescente sea una persona (a igual responsabilidad) mayor será su tendencia o probabilidad a responder afirmativamente al ítem.
Sin embargo, cuanto más responsable sea una persona (a igual aquiescencia) menor será su tendencia (o probabilidad) a responder afirmativamente.
En ese caso, tendríamos un ítem con un valor negativo de discriminación en la dimensión de responsabilidad, y uno positivo en la de aquiescencia.
:::

## Generalization: Inverse items

<br>

$$
  MDISC_i = \sqrt{a_{1i}^2 + ... + a_{ni}^2}
$$

::: fragment
$$
  D_i = \frac{-d_i}{MDISC_i}
$$
:::

::: fragment
$$
  \cos \, \mathbf{\alpha}_i = \frac{\mathbf{a}_i}{MDISC_i}
$$
:::

::: notes
Si vemos la expresión de los parámetros multidimensionales, podemos prevér cómo van a ser en estos casos.
Aquí he preferido representar la discriminación como suma de cuadrados, en lugar de como el producto matricial, pero es lo mismo, ¿os acordáis?
El primer elemento de un vector por el primero del otro, y asi sucesivamente...
todos sumados.
Sólo que en este caso el primer y el segundo vector son el mismo, así que es la suma de los elementos del vector al cuadrado.
Por lo tanto, la discriminación multidimensional siempre va a ser positiva.
¿Lo veis claro?

Por otro lado, la distancia (con signo), al depender sólo de la intersección y la discriminación multidimensional, nos da igual que los valores de la discriminación sean negativos.
Siempre va a cumplir que su desplazamiento es "inverso" al valor indicado por la intersección, como indicábamos antes.

La dirección sin embargo tiene una peculiaridad, y es que vemos que cada uno de los cosenos puede tener un signo distinto, que va a ser el mismo que el del valor de la discriminación para esa dimensión.
Eso lo que va a hacer es que el sentido del item en cada una de esas dimensiones venga determinado por el signo de ese valor.
:::

## Generalization: Inverse items {.smaller}

::::::::: columns
::::::: column
::: {.r-stack style="color:#5c6eb1;"}
$$
  \begin{matrix}
    \mathbf{a}_1 &=
      \begin{bmatrix}
        1 \\
        1
      \end{bmatrix} &
    d_1 = 0
  \end{matrix}
$$
:::

::: {.r-stack style="color:#90214a;"}
$$
  \begin{matrix}
    \mathbf{a}_2 &=
      \begin{bmatrix}
         1 \\
        -1
      \end{bmatrix} &
    d_2 = 0
  \end{matrix}
$$
:::

::: {.r-stack style="color:#749f4c;"}
$$
  \begin{matrix}
    \mathbf{a}_3 &=
      \begin{bmatrix}
        -1 \\
        -1
      \end{bmatrix} &
    d_3 = 0
  \end{matrix}
$$
:::

::: {.r-stack style="color:#d76f47;"}
$$
  \begin{matrix}
    \mathbf{a}_4 &=
      \begin{bmatrix}
        -1 \\
         1
      \end{bmatrix} &
    d_4 = 0
  \end{matrix}
$$
:::
:::::::

::: column
```{r inverse-item-representation}
#| fig-align:  center
#| fig-height: 8
#| fig-width:  8

mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,      1,    1,   0,
  2,      1,   -1,   0,
  3,     -1,   -1,   0,
  4,     -1,    1,   0
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
  rename_with(str_remove, pattern = "transf_")

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-1.5, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-1.5, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = ITEMS_PALETTE, guide = NULL) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title        = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = .5),
    axis.title.y.right = element_markdown(vjust = .5),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    ),
  )
```
:::
:::::::::

::: notes
Veamos un ejemplo: En el ítem 1 ambos cosenos son positivos, luego ambos ánculos son menores de 90º.
El ítem está en el primer cuadrante.

El segundo ítem va a tener un coseno positivo (ángulo menor que 90º) y uno negativo, por lo que será mayor que 90º; por tanto, apunta en el sentido negativo de ese eje.
El ángulo es 180º menos el ángulo que tendría ese mismo coseno si fuera positivo.

En el tercer ítem pasa lo mismo, pero con los dos cosenos negativos (y por tanto, ángulos mayores que 90º), y en el cuarto como el segundo pero al revés.

Es decir, las propiedades algebraicas de la discriminación determinan las propiedades geométricas; y vemos cómo se puede visualizar gráficamente con facilidad.
:::

## Generalization: Inverse items {.smaller}

::::::::: columns
::::::: column
::: {.r-stack style="color:#5c6eb1;"}
$$
  \begin{matrix}
    \mathbf{a}_1 &=
      \begin{bmatrix}
        1 \\
        1
      \end{bmatrix} &
    d_1 = 0
  \end{matrix}
$$
:::

::: {.r-stack style="color:#90214a;"}
$$
  \begin{matrix}
    \mathbf{a}_2 &=
      \begin{bmatrix}
         1 \\
        -1
      \end{bmatrix} &
    d_2 = -0.5
  \end{matrix}
$$
:::

::: {.r-stack style="color:#749f4c;"}
$$
  \begin{matrix}
    \mathbf{a}_3 &=
      \begin{bmatrix}
        -1 \\
        -1
      \end{bmatrix} &
    d_3 = -0.5
  \end{matrix}
$$
:::

::: {.r-stack style="color:#d76f47;"}
$$
  \begin{matrix}
    \mathbf{a}_4 &=
      \begin{bmatrix}
        -1 \\
         1
      \end{bmatrix} &
    d_4 = 0.25
  \end{matrix}
$$
:::
:::::::

::: column
```{r inverse-item-representation-distance}
#| fig-align:  center
#| fig-height: 8
#| fig-width:  8

mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,      1,    1,    0,
  2,      1,   -1,   -0.5,
  3,     -1,   -1,   -0.5,
  4,     -1,    1,    0.25
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
  rename_with(str_remove, pattern = "transf_")

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-1.5, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-1.5, 1.5),
    breaks       = -1:2,
    minor_breaks = NULL,
    name         = NULL,
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = ITEMS_PALETTE, guide = NULL) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title        = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = .5),
    axis.title.y.right = element_markdown(vjust = .5),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    ),
  )
```
:::
:::::::::

::: notes
Si la intersección es distinta de cero, como decíamos antes: El desplazamiento del ítem es opuesto al sentido en el que apunta el ítem.

Si la intersección es negativa, como en los ítems 2 y 3, el vector se desplaza "hacia adelante" en el sentido de la cabeza de la flecha.
Es decir, el ítem es "más difícil en general, o menos atractivo", y la región donde es relativamente fácil acertar el ítem (o estar de acuerdo con él) disminuye.

Si la intersección es negativa, como en el ítem 1, el vector se desplaza "hacia atrás"; o sea, el ítem es "en general más fácil o atractivo": La región donde es fácil acertar o estar de acuerdo con el ítem se amplía.

Y da igual en qué dirección apunten los vectores; esto se cumple en todos los casos.

Esta generalización es bastante evidente, y se puede aplicar sin problema a ítems no cognitivos.
Sin embargo, hay que tener en cuenta que Reckase explicitaba como condición que los ítems tenían que ser "monotonamente crecientes".
:::

## "Inverse" items in the literature {.smaller}

:::::: columns
:::: {.column width="62%"}
> \[...\] the negative relation of Item 15 with geometry achievement is particularly puzzling.

---@mcdonald_basis_2000 [p. 109]

::: fragment
> This result occurs because $a_1$ for the item is negative, and thus as an examinee's $\theta_1$ ability increases, the chance of answering the item correctly actually decreases.

---@ackerman_multidimensional_2005-1 [p. 17]
:::
::::

::: {.column width="38%"}
<!-- TODO: Annotate figure? -->

!["ACT Mathematics Usage Test" items](../www/Fig_1-6_Contemporary_Psychometrics.png){fig-align="left"}
:::
::::::

::: {.fragment .r-center}
### Generalization: "Constant monotony"
:::

::: aside
Figure taken from @ackerman_multidimensional_2005-1
:::

::: notes
Pero en la literatura nos encontramos casos como estos, que no lo son, y a los autores no se les ocurre decir que "hay que generalizar esas definiciones".
Más bien, hacen comentarios como estos, restándole importancia al hecho de que los parámetros de discriminación sean negativos ("particularmente misterioso"),...

\[NEXT\]

o afirmando lo obvio (el parámetro de discriminación del ítem es negativo para una dimensión, por lo que la probabilidad de respuesta correcta disminuye al incrementarse su habilidad)

\[NEXT\]

En lugar de eso, proponemos que los supuestos de Reckase se "relajen" para considerar ítems con probabilidad "con monotonía constante" (es decir, que no tengan un "pico" u óptimo de rasgo latente para dar una respuesta positiva y luego decrezca), sino que cuanto más extremo sea el nivel de rasgo, más extrema es también la probabilidad (positiva o negativa).
Este caso general incluye también la probabilidad constante (ya que es monotonía "nula constante", es decir, "el cambio es nulo de manera constante").
Este tipo de ítems, positivos, negativos, o nulos, se pueden acomodar y modelar fácilmente por un modelo como el logístico de 2 parámetros multidimensional, como hemos visto.
Ítems que estarían excluidos, por no tener monotonía constante, podrían ser formulaciones como, por ejemplo, "Soy tan extrovertido como la mayoría de mis amigos" (alguien puede decir, "No, soy más introvertido que todos ellos", respuesta negativa con rasgo bajo, o "Qué va, soy el más extrovertido con diferencia", respuesta negativa con rasgo alto).

Bien, esto por lo que respecta a los ítems que denominamos "inversos" (o que tienen parámetros de discrimiinación / pesos factoriales inversos).
Como veis, esta generalización es necesaria cuando hablamos de medición en el dominio "no cognitivos"; en el cognitivo, ya vimos que no tenía sentido, y de ahí que hiciera falta esta matización.
:::

## Assumptions: Orthogonality {.smaller}

> \[...\] before performing the differentiation, the constraint that Σ cos^2^**α** = 1 is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

::::: columns
::: {.column .fragment}
```{r oblique-rotation-axis-1}
LIM_INF_X <- -2.7
LIM_INF_Y <- -0.5
LIM_SUP   <-  2.7

axis_lims_x <- c(LIM_INF_X, LIM_SUP)
axis_lims_y <- c(LIM_INF_Y, LIM_SUP)

point <- tibble(F1 = .1, F2 = .3)

point_spikes <- point      |>
  add_column(type = "end")                      |> 
  bind_rows(tibble(F1 = 0, F2 = 0))             |>
  complete(F1, F2, fill = list(type = "start")) |>
  filter(F1 != 0 | F2 != 0)

point_spikes <- point_spikes                           |> 
  bind_rows(point_spikes |> slice(3))                  |>
  mutate(n = paste0('F', (row_number() + 1) %% 2 + 1)) |>
  pivot_wider(names_from = type, values_from = F1:F2)

point |>
  ggplot(mapping = aes(F1, F2))            +
  geom_segment(
    data    = point_spikes,
    mapping = aes(F1_start, F2_start, xend = F1_end, yend = F2_end),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43',
  )                                        +
  geom_point(size = I(4))                  +
  scale_x_continuous(
    limits = axis_lims_x,
    breaks = 0,
    labels = NULL,
    name   = NULL
  )                                        +
  scale_y_continuous(
    limits = axis_lims_y,
    breaks = 0,
    labels = NULL,
    name   = NULL
  )                                        +
  coord_fixed(expand = FALSE, clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      color     = "black",
      linewidth = GRID_WIDTH
    )
  )
```
:::

::: column
:::
:::::

::: notes
Ahora, la TRI multidimensional también dijimos que se interesaba sobre todo por medir (y ordenar) a las personas, no tanto por interpretar sustantivamente esas dimensiones de rasgo latente.
De ahí que estimar un modelo con suficientes dimensiones para explicar la variabilidad entre los sujetos fuese importante, pero no era tan importante cuáles fueran esas dimensiones.
Por eso, hay un supuesto, implícito muchas veces en TRI, de que las dimensiones no están correlacionadas: Se estima el modelo con N dimensiones, las que haga falta, añadiendo restricciones a los ítems para identificarlas, y eso da lugar a N dimensiones independientes entre sí (con correlaciones de 0), representadas en N ejes ortogonales (ángulos rectos todos ellos).

\[NEXT\]

Que esos ejes representen competencia o habilidad en esto o lo otro, en principio, nos da igual.
Con asegurarnos de que hay suficientes dimensiones para "capturar" toda la variabilidad debida a esos rasgos nos basta.

Pues bien, ese es el otro supuesto que hace Reckase para calcular los parámetros multidimensionales: Los cosenos directores del ítem al cuadrado suman 1.
Es decir, está aplicando el teorema de Pitágoras.
Esta expresión implica, de manera implícita, que todos los ejes del espacio latente son "ortogonales" entre sí.
Como digo, en TRI originalmente no importaba mucho que los ejes se pudieran interpretar como rasgos latentes.
No obstante, en 1999 llega McDonald, y dice que la TRI y el AF son fundamentalmente la misma cosa.
:::

## Assumptions: Orthogonality {.smaller}

<!-- TODO: Equation as LaTeX -->

> ~~\[...\] before performing the differentiation, the constraint that Σ cos^2^**α** = 1 is added to the expression for the slope \[...\]~~

---@reckase_difficulty_1985 [p. 404]

::::: columns
::: column
<!-- TODO: Animate? -->

```{r oblique-rotation-axis-2}
SEC_COLOR <- "grey60"

BASIS_VEC_1 <- c(1, 0)
BASIS_VEC_2 <- c(-8, 1)
BASIS_VEC_2 <- BASIS_VEC_2 / sqrt(sum(BASIS_VEC_2^2))

axis_slope <- BASIS_VEC_2[2] / BASIS_VEC_2[1]
transf_matrix <- cbind(BASIS_VEC_1, BASIS_VEC_2) |> unname()

point_transf <- point |>
  pivot_longer(everything()) |>
  mutate(value = solve(transf_matrix) %*% value |> drop()) |>
  pivot_wider()

point_spikes_transf <- point_transf             |>
  add_column(type = "end")                      |>
  bind_rows(tibble(F1 = 0, F2 = 0))             |>
  complete(F1, F2, fill = list(type = "start")) |>
  filter(F1 != 0 | F2 != 0)

point_spikes_transf <- point_spikes_transf             |>
  bind_rows(point_spikes_transf |> slice(3))           |>
  mutate(n = paste0('F', (row_number() + 1) %% 2 + 1)) |>
  pivot_wider(names_from = type, values_from = F1:F2)

point_spikes_transf_ob <- point_spikes_transf |>
  pivot_longer(starts_with("F")) |>
  separate(name, into = c("dim", "coord")) |>
  pivot_wider(names_from = coord, values_from = value) |>
  group_by(n) |>
  mutate(
    start = (transf_matrix %*% start |> drop()),
    end   = (transf_matrix %*% end   |> drop())
  ) |>
  pivot_wider(names_from = dim, values_from = start:end)

point |>
  ggplot(mapping = aes(F1, F2)) +
  geom_abline(slope = axis_slope, linewidth = GRID_WIDTH) +
  geom_segment(
    data    = point_spikes,
    mapping = aes(F1_start, F2_start, xend = F1_end, yend = F2_end),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43',
    color = SEC_COLOR
  ) +
  geom_segment(
    data    = point_spikes_transf_ob,
    mapping = aes(start_F1, start_F2, xend = end_F1, yend = end_F2),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43'
  ) +
  geom_point(size = I(4)) +
  scale_x_continuous(
    limits = axis_lims_x,
    breaks = 0,
    labels = NULL,
    name   = NULL
  ) +
  scale_y_continuous(
    limits = axis_lims_y,
    breaks = 0,
    labels = NULL,
    name   = NULL
  ) +
  coord_fixed(expand = FALSE, clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.ticks         = element_blank(),
    # panel.grid.major   = element_line(linewidth = GRID_WIDTH),
    panel.grid.major.x = element_line(
      linewidth = GRID_WIDTH,
      color = SEC_COLOR
    ),
    panel.grid.major.y = element_line(linewidth = GRID_WIDTH, color = "black")
  )
```
:::

::: column
:::
:::::

::: notes
Pero en AF tenemos dimensiones correlacionadas la mayoría de las veces, y eso como hemos visto implica hacer rotaciones oblicuas, que dan lugar a ejes no ortogonales.

Aquí vemos un ejemplo de ejes oblicuos (muy exagerado, eso sí).
Imaginad que esto que estamos representando aquí es un punto sobre La Tierra, y sus coordenadas rectangulares son la longitud y latitud.

Es posible que por algún motivo (no sé cuál, pero imaginémoslo) nos interesen las coordenadas en estos "ejes oblicuos": Este-Oeste, y uno que podría ser algo así como "Este-Sureste este este - Oeste-Noroeste oeste oeste".
¿Se entiende claramente?
Proyectando cada coordenada, de forma paralela a los ejes, obtendríamos las coordenadas en este nuevo sistema de "coordenadas geodésicas oblicuas".

Luego esta restricción que propone Reckase parece que en principio no se podría aplicar.
:::

## (But, an aside...) {.smaller}

::::: columns
::: column
![A red cube in isometric perspective](../www/slideshow-assets/isometric-cube.jpg){fig-align="center"}
:::

::: {.column .fragment}
<br>

> It has been suggested by some researchers that the angle between the axes represents the degree of correlation.
> However, for sake of clarity, an orthogonal axes system is used in which distance measures and vectors can be easily calculated, understood, and interpreted.

---@ackerman_multidimensional_2005-1 [p. 16]
:::
:::::

::: aside
Figure taken from [Vecteezy](https://www.vecteezy.com/vector-art/3709640-isometric-cube-on-a-white-background)
:::

::: notes
Sin embargo, mirad este ejemplo:

¿Qué veis?
¿Un cubo, verdad?
¿Son sus lados todos iguales?
¿Y sus ángulos?
Son todos ángulos rectos, se entiende.
Pero si cojo un transportador de ángulos y mido, voy a ver que este mide 120 grados, y este 60.

Este cubo está representado en un plano en perspectiva isométrica (Creo que todo el mundo sabe lo que es la perspectiva isométrica, si no de dibujo en bachillerato, de jugar a los SIMS, aunque sea).

Bien, si ahora os pregunto por las diagonales de una cara, ¿diríais que son iguales?
Y sin embargo, si mido (en este plano) las dos diagonales, vemos que son distintas.
¿Estáis de acuerdo?

Esta representación no es errónea: De hecho, se necesita algún sistema de perspectiva para representar un cuerpo sólido en dos dimensiones; la isométrica es solamente una de ellas.

Es decir, que nada me impide, si lo necesito, representar unos ejes con un ángulo cualquiera (recto en este caso) como otro ángulo (120 grados).
:::

## Assumptions: Orthogonality {.smaller}

> \[...\] before performing the differentiation, the constraint that Σ cos^2^**α** = 1 is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

::::: columns
::: column
```{r oblique-rotation-axis-2}
```
:::

::: {.column .fragment}
<!-- TODO: Animate? -->

```{r oblique-rotation-rect-coords}
point_transf |>
  ggplot(mapping = aes(F1, F2))            +
  geom_segment(
    data    = point_spikes_transf,
    mapping = aes(F1_start, F2_start, xend = F1_end, yend = F2_end),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43',
  )                                        +
  geom_point(size = I(4))                  +
  scale_x_continuous(
    limits = axis_lims_x,
    breaks = 0,
    labels = NULL,
    name   = NULL
  )                                        +
  scale_y_continuous(
    limits = axis_lims_y,
    breaks = 0,
    labels = NULL,
    name   = NULL
  )                                        +
  coord_fixed(expand = FALSE, clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      color     = "black",
      linewidth = GRID_WIDTH
    )
  )
```
:::
:::::

::: notes
Lo mismo pasa al representar los ejes de una rotación oblicua: Nada me impide representar las coordenadas resultantes en ejes rectangulares (las llamadas "coordenadas Cartesianas rectangulares").
Son coordenadas, nada más; el espacio en el que yo las represente en realidad da igual.
Y de hecho, esto lo habréis visto muchas veces; por ejemplo, SPSS representa los pesos factoriales siempre en coordenadas rectangulares, incluso después de aplicar una rotación oblicua.

Así que si represento un ítem en ejes rectangulares, nada me impide aplicar las fórmulas de Reckase y obtener sus parámetros multidimensionales.

Pero algo parece fallar cuando generalizamos los parámetros multidimensionales de TRI que hemos mostrado antes a esos espacios de coordenadas no ortogonales.
Si rotamos los ejes de coordenadas, pero los ítems siguen siendo los mismos, es de esperar que la discriminación multidimensional de un ítem (i.e., la "longitud" de su vector) no cambie.
Esta propiedad se llama invarianza.
Veámoslo con un ejemplo.
:::

## Assumptions: Orthogonality {.smaller}

::::::: columns
::: column
```{r item-oblique-axes-1}
#| fig-align:  center
#| fig-height: 8
#| fig-width:  8
mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,     0.5,  1,    0
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
    rename_with(str_remove, pattern = "transf_")

item_spikes <- mirt_item_coords        |>
  slice(1 |> rep(2))                   |>
  add_column(dim = 'F' |> paste0(1:2)) |>
  mutate(
    origin_1 = if_else(dim == 'F1', end_1, origin_1),
    origin_2 = if_else(dim == 'F2', end_2, origin_2),
  )

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    data = item_spikes,
    mapping = aes(origin_1, origin_2, xend = end_1, yend = end_2),
    linewidth = LINE_WIDTH,
    linetype  = '43',
    color     = PALETTE_BLUE["light"]
  ) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-1:3)/2,
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-1:2)/2,
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = unname(PALETTE_BLUE["normal"]), guide = NULL) +
  annotate(
    "richtext",
    x = c(0.5, -0.1), y = c(-0.1, 1),
    label         = c("*a*^*^~11~", "*a*^*^~21~"),
    size          = 8,
    label.colour  = NA,
    label.padding = unit(0, "lines"),
    fill = NA
  ) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title         = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = 1/3),
    axis.title.y.right = element_markdown(vjust = 1/3),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    )
  )
```
:::

::::: column
::: fragment
$$
  \begin{align}
    \mathbf{a}^*_1 &=
      \begin{bmatrix}
        0.5 \\
        1
      \end{bmatrix}\\
      d^*_1 &= 0
  \end{align}
$$
:::

<br>

::: fragment
$$
  \begin{align}
    MDISC^*_1 &= \sqrt{
        [0.5, 1]
        \begin{bmatrix}
          0.5 \\
          1
        \end{bmatrix}
      } \\
      &= \sqrt{ 0.5 · 0.5 + 1 · 1 } \\
      &= \sqrt{ 0.25 + 1 } \\
      &= \sqrt{ 1.25 } \approx 1.12
  \end{align}
$$
:::
:::::
:::::::

::: notes
Supongamos un ítem, en coordenadas ortogonales (de una solución sin rotar).
(Con vuestro permiso, le voy a llamar "a asterico", para representar las coordenadas "originales" de una solución ortogonal, antes de una rotación).
El vector muestra los componentes de su parámetrosde discriminación en las dos dimensiones de los ejes representados.
Esto sería la "solución sin rotar" que resulta típicamente de ajustar un modelo de TRI multdimensional, o un análisis factorial exploratorio (si lo convertimos a métrica de TRI).
La discriminación multidimensional de este vector es (explicar paso a paso).

Supongamos que aplicamos una rotación oblicua.
Imaginad, por ejemplo, que la solución incluye más ítems aparte de este, y cada uno de los ejes se alinea con un grupo de ítems después de hacer la rotación, de forma que da una solución próxima a la estructura simple.
Esa rotación como digo es oblicua, da lugar a dos ejes no ortogonales, y cada eje se identifica con un rasgo latente sustantivo.
Como son oblicuos, la solución representa que esos dos rasgos latentes están correlacionados entre sí, con una correlación "la que sea", no nos importa cuál en este momento.
:::

## Assumptions: Orthogonality {.smaller}

:::::::: columns
::: column
```{r item-oblique-axes-2}
#| fig-align:  center
#| fig-height: 8
#| fig-width:  8
BASIS_VEC_1 <- c(1, 0)
BASIS_VEC_2 <- c(-.5, 1)

axis_slope <- BASIS_VEC_2[2] / BASIS_VEC_2[1]
transf_matrix <- cbind(BASIS_VEC_1, BASIS_VEC_2) |> unname()

item_coords_transf <- mirt_item_coords |>
  pivot_longer(-item) |>
  separate(name, into = c("coord", "dim")) |>
  group_by(coord) |>
  mutate(
    dim   = paste0('F', dim),
    value = solve(transf_matrix) %*% value |> drop()
  ) |>
  ungroup() |>
  pivot_wider(names_from = dim)

item_spikes_transf <- item_coords_transf         |>
  select(-item)                                  |>
  filter(coord == "end")                         |>
  bind_rows(tibble(F1 = 0, F2 = 0))              |>
  complete(F1, F2, fill = list(coord = "start")) |>
  filter(F1 != 0 | F2 != 0)

item_spikes_transf <- item_spikes_transf               |>
  bind_rows(item_spikes_transf |> slice(3))            |>
  mutate(n = paste0('F', (row_number() + 1) %% 2 + 1)) |>
  pivot_wider(names_from = coord, values_from = F1:F2)

item_spikes_transf_ob <- item_spikes_transf |>
  pivot_longer(starts_with("F")) |>
  separate(name, into = c("dim", "coord")) |>
  pivot_wider(names_from = coord, values_from = value) |>
  group_by(n) |>
  mutate(
    start = (transf_matrix %*% start |> drop()),
    end   = (transf_matrix %*% end   |> drop())
  ) |>
  pivot_wider(names_from = dim, values_from = start:end)

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  ) +
  geom_abline(
    intercept = -2:3,
    slope     = -2,
    linewidth = GRID_WIDTH,
    linetype  = "17",
    color     = AXIS_COLOR
  ) +
  geom_abline(slope = -2, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0,  linewidth = GRID_WIDTH) +
  geom_segment(
    data = item_spikes_transf_ob,
    mapping = aes(start_F1, start_F2, xend = end_F1, yend = end_F2),
    inherit.aes = FALSE,
    linewidth = LINE_WIDTH,
    linetype  = '43',
    color     = PALETTE_BLUE["light"]
  ) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-2:2)/2 + .375,
    labels       = ~number(. - .375, .1),
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-1:2)/2,
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = unname(PALETTE_BLUE["normal"]), guide = NULL) +
  annotate(
    "richtext",
    x = c(1, -0.59), y = c(-0.07, 1),
    label         = c("*a*~11~", "*a*~21~"),
    size          = 8,
    label.colour  = NA,
    label.padding = unit(0, "lines"),
    fill = NA
  ) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title         = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = 0),
    axis.title.y.right = element_markdown(vjust = 1/3),
    axis.ticks         = element_blank(),
    panel.grid.major.y = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    )
  )
```
:::

:::::: column
::: fragment
$$
  \mathbf{Q} = \begin{bmatrix}
      1 & -0.5 \\
      0 & 1
    \end{bmatrix}; \quad
  \mathbf{Q}^{-1} = \begin{bmatrix}
      1 & 0.5 \\
      0 & 1
    \end{bmatrix}
$$
:::

::: fragment
$$
  \begin{align}
    \mathbf{a}_1 =
    \mathbf{Q}^{-1} \mathbf{a}^*_1 &= \begin{bmatrix}
      1 & 0.5 \\
      0 & 1
    \end{bmatrix}
    \begin{bmatrix}
      0.5 \\
      1
    \end{bmatrix} \\
    &= \begin{bmatrix}
      1 · 0.5 + 0.5 · 1 \\
      0 · 0.5 + 1 · 1
    \end{bmatrix} \\
    &= \begin{bmatrix}
      0.5 + 0.5 \\
      0   + 1
    \end{bmatrix}
    = \begin{bmatrix}
      1 \\
      1
    \end{bmatrix}
  \end{align}
$$
:::

::: fragment
$$
  \begin{align}
    \mathbf{a}_1 &=
      \begin{bmatrix}
        1 \\
        1
      \end{bmatrix}\\
      d_1 &= d^*_1 = 0
  \end{align}
$$
:::
::::::
::::::::

::: notes
Los ejes resultantes son los que se muestran en este gráfico.
No es exactamente una rotación, ya que el eje oblicuo \[señalar\] es un poco más largo (si lo rotáramos solamente acabaría por aquí \[señalar\]), pero para el caso es una transformación de los ejes, y cualquier transformación lineal nos sirve para ejemplificar, y así simplificamos el ejemplo.
Los nuevos valores del parámetros de discriminación del ítem, en este caso ya sí lo llamo "a", porque esta es la solución rotada, la que me interesa, la que es interpretable según mi teoría, están representados cada uno por la "proyección" de ese vector sobre cada uno de los ejes (proyección paralela al otro eje, u otros, si hubiera más de dos).

Hacer una rotación supone "multiplicar" las coordenadas de este vector por la matriz de rotación que representa ese nuevo sistema de coordenadas (la transformación del ortogonal a este nuevo, oblicuo).

\[NEXT\]

En este caso, la matriz de rotación es esta (la llamamos matriz Q).
Esta es la matriz que convierte las coordenadas del espacio ortogonal original a las coordenadas transformadas (coordenadas "rotadas").

\[NEXT\]

Como los vectores se representan en columnas, se "pre-multiplica" por la matriz de rotación.
Esto es parecido a multiplicar por un vector; es como si cada fila de la matriz fuese un vector, y los resultados se ponen cada uno en una fila.
\[Explicar paso a paso el producto matricial\]

\[NEXT\]

El resultado es, como vemos en el gráfico, (1, 1).
El parámetro de discriminación de este ítem, en su solución rotada, interpretable, es este, (1, 1).
El valor de la intersección del modelo no cambia.
:::

## Assumptions: Orthogonality {.smaller}

:::::::: columns
::: column
```{r item-oblique-axes-3}
#| fig-align:  center
#| fig-height: 8
#| fig-width:  8
mirt_item <- tribble(
  ~item, ~a_1, ~a_2, ~d,
  1,     1,    1,    0
)

mirt_item_params <- mirt_item |>
  compute_mirt_params(d, starts_with('a'))

mirt_item_coords <- mirt_item_params |>
  compute_mirt_coords(D, MDISC, starts_with("cos"), original_coords = FALSE) |>
    rename_with(str_remove, pattern = "transf_")

item_spikes <- mirt_item_coords        |>
  slice(1 |> rep(2))                   |>
  add_column(dim = 'F' |> paste0(1:2)) |>
  mutate(
    origin_1 = if_else(dim == 'F1', end_1, origin_1),
    origin_2 = if_else(dim == 'F2', end_2, origin_2),
  )

mirt_item_coords |>
  ggplot(
    aes(
      origin_1, origin_2,
      xend = end_1,
      yend = end_2,
      color = factor(item), fill = factor(item)
    ),
  )                                                  +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_segment(
    data = item_spikes,
    mapping = aes(origin_1, origin_2, xend = end_1, yend = end_2),
    linewidth = LINE_WIDTH,
    linetype  = '43',
    color     = PALETTE_BLUE["light"]
  ) +
  geom_segment(
    arrow     = arrow(angle = 20, length = unit(10, "points"), type = "closed"),
    linejoin  = "mitre",
    linewidth = VECTOR_WIDTH
  ) +
  scale_x_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-1:3)/2,
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    limits       = c(-0.75, 1.5),
    breaks       = (-1:2)/2,
    minor_breaks = NULL,
    name         = NULL,
    expand       = expansion(),
    sec.axis     = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  scale_color_manual(values = unname(PALETTE_BLUE["normal"]), guide = NULL) +
  annotate(
    "richtext",
    x = c(1, -0.1), y = c(-0.1, 1),
    label         = c("*a*~11~", "*a*~21~"),
    size          = 8,
    label.colour  = NA,
    label.padding = unit(0, "lines"),
    fill = NA
  ) +
  coord_fixed(clip = "on") +
  theme(
    axis.line          = element_blank(),
    axis.title         = element_markdown(size = 20, padding = unit(0, "lines")),
    axis.title.x       = element_markdown(hjust = 1/3),
    axis.title.y.right = element_markdown(vjust = 1/3),
    axis.ticks         = element_blank(),
    panel.grid.major   = element_line(
      linewidth = GRID_WIDTH,
      linetype  = "17"
    )
  )
```
:::

:::::: column
::: fragment
$$
  \mathbf{a}_1 = \begin{bmatrix}
    1 \\
    1
  \end{bmatrix};\quad
  d_1 = 0
$$
:::

::: fragment
> Using orthogonal axes, discrimination corresponds to the length of the item response vector

---@ackerman_multidimensional_2005 [p. 6]
:::

::: fragment
$$
  \begin{align}
    MDISC_1 &= \sqrt{
        [1, 1]
        \begin{bmatrix}
          1 \\
          1
        \end{bmatrix}
      } \\
      &= \sqrt{ 1 · 1 + 1 · 1 } \\
      &= \sqrt{ 1 + 1 } \\
      &= \sqrt{ 2 } \approx 1.41
  \end{align}
$$
:::
::::::
::::::::

::: notes
Ahora bien, al igual que vimos antes, nadie me impide representar este ítem en coordenadas rectangulares.

\[NEXT\]

Y si el ítem está representado en coordenadas rectangulares, puedo aplicar, como vimos, las fórmulas de Reckase.
Pero si hago eso, me encuentro con que el resultado de la discriminación es este, raíz de 2.
Antes, recordaréis que era raíz de 1,25, aproximadamente 1,12.

Es decir, ¿que el ítem discrimina mejor o peor en función de qué ejes quiera yo considerar?
El ítem es exactamente el mismo, y por lo tanto su discriminación (multidimensional al menos, ya que no depende de la "dirección" a considerar, como los valores del vector de discriminación) debería ser la misma.
Esta propiedad se llama "invarianza" del ítem.
:::

## Assumptions: Orthogonality

<!-- TODO: ACT en McDonald, 1999? -->

::: r-center
!["ACT Mathematics Usage Test" items](../www/Fig_1-6_Contemporary_Psychometrics.png){fig-align="center" height="400px"}
:::

::: aside
Figure taken from @ackerman_multidimensional_2005-1
:::

::: notes
No tiene sentido que cambie su discriminación multidimensional, y sin embargo la literatura está llena de ejemplos que calculan este parámetro con la fórmula de Reckase (en un espacio de "rasgos correlacionados", que en seguida veremos por qué eso es importante), y que no se preocupan por "los ejes" que representan esos rasgos de manera fidedigna.

Para la representación gráfica esto puede valer, como dice Ackerman aquí.
Pero a la hora de calcular los parámetros (y Ackerman lo hace en este capítulo de libro), solamente considerar el caso ortogonal, la información que obtenemos resulta deficiente; obtenemos parámetros de discriminación multidimensional "distorsionados".
:::

## Generalization: Oblique axes {.smaller}

> \[...\] before performing the differentiation, the constraint that Σ cos^2^**α** = 1 is added to the expression for the slope \[...\]

---@reckase_difficulty_1985 [p. 404]

:::::::::::: columns
:::::: column
::::: r-stack
::: {.fragment fragment-index="1"}
```{r corr-traits-example}
#| fig-width:  5.5
#| fig-height: 5.5

set.seed(7117)

corr_matrix <- matrix(c(1, .5, .5, 1), nrow = 2)

thetas <- rmvnorm(10^4, sigma = corr_matrix) |>
  as_tibble(.name_repair = ~paste0("trait_", 1:2)) |>
    rownames_to_column("i")

thetas |>
  ggplot(mapping = aes(trait_1, trait_2)) +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_point(alpha = .33, shape = 16, colour = I(PALETTE_BLUE["normal"])) +
  scale_x_continuous(
    minor_breaks = NULL,
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*~2~", labels = NULL)
  ) +
  scale_y_continuous(
    minor_breaks = NULL,
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*~1~", labels = NULL)
  ) +
  coord_fixed() +
  theme(
    axis.ticks       = element_blank(),
    axis.line        = element_blank(),
    axis.title       = element_markdown(),
    axis.title.y     = element_markdown(),
    panel.grid.major = element_line(linetype = "17", linewidth = GRID_WIDTH)
  )
```
:::

::: {.fragment .fade-in fragment-index="4"}
```{r corr-traits-transf-example}
#| fig-width:  5.5
#| fig-height: 5.5

# This is a square root of the correlation matrix, then inverted and transposed:
orth_matrix <- c(1, 0, .5, sqrt(.75)) |> matrix(nrow = 2) |> solve() |> t()

thetas_transf <- thetas |>
  pivot_longer(starts_with("trait")) |>
  group_by(i) |>
  mutate(value = orth_matrix %*% value |> drop()) |>
  pivot_wider() |>
  ungroup()

thetas_transf |>
  ggplot(mapping = aes(trait_1, trait_2)) +
  geom_vline(xintercept = 0, linewidth = GRID_WIDTH) +
  geom_hline(yintercept = 0, linewidth = GRID_WIDTH) +
  geom_point(alpha = .33, shape = 16, colour = I(PALETTE_BLUE["normal"])) +
  scale_x_continuous(
    minor_breaks = NULL,
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*^*^~2~", labels = NULL)
  ) +
  scale_y_continuous(
    minor_breaks = NULL,
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "*&theta;*^*^~1~", labels = NULL)
  ) +
  coord_fixed() +
  theme(
    axis.ticks       = element_blank(),
    axis.line        = element_blank(),
    axis.title       = element_markdown(),
    axis.title.y     = element_markdown(),
    panel.grid.major = element_line(linetype = "17", linewidth = GRID_WIDTH)
  )
```
:::
:::::
::::::

::::::: column
::: {.fragment fragment-index="1"}
$$
  cor (\mathbf{\theta}) = \mathbf{R}
$$
:::

::: {.fragment fragment-index="2"}
$$
  \mathbf{R} = \begin{bmatrix}
      1 & 0.5 \\
      0.5 & 1
    \end{bmatrix}
$$
:::

::: {.fragment fragment-index="3"}
$$
  \mathbf{\theta}^* = \mathbf{P} \mathbf{\theta}
$$
:::

::: {.fragment fragment-index="4"}
$$
  cor (\mathbf{\theta}^*) =
    \mathbf{R}^* =
    \begin{bmatrix}
      1 & 0 \\
      0 & 1
    \end{bmatrix}
$$
:::
:::::::
::::::::::::

::: notes
Hacer esta generalización no es algo tan directo como ocurría con la probabilidad "monótonamente decreciente".
Si os acordáis, Reckase aplicaba esta restricción de que la suma de cosenos directores era 1 (que se deriva del teorema de Pitágoras).
Pero para aplicarla hace falta asumir que esos cosenos directores son todos ortogonales entre sí, como ya hemos visto.
Si no tenemos ángulos rectos, esa condición no se cumple.

\[NEXT\]

Y lo que nosotros estamos asumiendo es que tenemos una distribución de vectores de rasgo latente ($\theta$) con una matriz de correlaciones $R$ (la que sea).

\[NEXT\]

(En este ejemplo, vamos a usar una distribución normal estandarizada con correlación de 0,5).

Normalmente representamos esos vectores en coordenadas rectangulares (ejes ortogonales), pero para obtener los parámetros multidimensionales generalizados a ejes oblicuos tenemos que representados en una base no ortogonal (aquí también hablaría de "unidades no estandarizadas", es decir, que la distancia entre el 0 y el 1 pueda ser "distinta de la base estándar, pero digamos solamente "no ortogonal" por simplificar).

¿Cómo podríamos solucionar esto?

\[NEXT\]

Lo que podemos hacer es aplicar una "transformación" a los rasgos latentes de forma que pasemos de esa base oblicua (la que sea, aún no sabemos cuál es) a una base ortogonal.
Esto es una transformación de las coordenadas, como la que aplicamos antes para calcular las coordenadas del ítem tras aplicar la rotación oblicua, pero aplicada en este caso al vector de rasgo latente.

Ahora bien, como recordaréis, queremos representar los ítems en unos ejes que sean, por los ángulos relativos entre ellos, una representación fidedigna, de alguna manera, de las correlaciones entre las dimensiones de rasgo latente.

Estas nuevas coordenadas (transformadas), que ahora están en una base ortogonal, queremos que representen una distribución en la que las correlaciones sean nulas, es decir, que la matriz de correlaciones transformada ($R^*$ la vamos a llamar) sea una matriz identidad (unos en la diagonal, ceros en el resto).
Pero hay que tener en cuenta que esto es un "artefacto matemático".
Estamos haciendo esto por conveniencia, pero las coordenadas que realmente nos interesan como investigadores, porque son las que verdaderamente representan algo que nos interesa investigar, son esas coordenadas originales, que pueden representar cualquier fenómeno que sea de nuestro interés (rasgos de personalidad, síntomas, actitudes, inteligencia, conocimientos... lo que estemos investigando).

Llegados a este punto, la incógnita es: ¿Qué matriz es $\mathbf{P}$, es decir, la matriz que nos permite pasar de las coordenadas originales, $\mathbf{\theta}$, a las transformadas $\mathbf{\theta^*}$?
Esto es equivalente a decir, ¿cuáles son los ejes originales en los que tenemos que representar $\mathbf{\theta}$?
(Recordad, para que $\mathbf{R^*}$ sea una matriz identidad, 1's en la diagonal y 0's fuera de ella).
:::

## Generalization: Oblique axes {.smaller}

<center>

<br>

[bit.ly/vote-coords](https://bit.ly/vote-coords) ([slido.com](https://slido.com) : #1904926)

<br>

![](../www/slideshow-assets/bit.ly_vote-coords.svg){width="40%" height="40%"}

</center>

::: notes
En base a lo planteado anteriormente, me gustaría hacer una prueba con vosotras y vosotros.
Aquí tenéis un enlace y QR para participar en una breve "encuesta".
:::

## Generalization: Oblique axes {.smaller}

<br>

<!-- # TODO: Faltan dos primeros paneles -->

:::::: columns
::: {.column width="33%"}
```{r}
#| fig-width:  5.5
#| fig-height: 5.5
thetas_transf |>
  ggplot(mapping = aes(trait_1, trait_2)) +
  geom_point(alpha = .33, shape = 16, colour = I(PALETTE_BLUE["normal"])) +
  scale_x_continuous(
    minor_breaks = NULL,
    labels = \(breaks) '' |> rep(length(breaks)),
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL,
    sec.axis = dup_axis(name = "{*&theta;*~1~, *&theta;*~2~}", labels = NULL)
  ) +
  scale_y_continuous(
    minor_breaks = NULL,
    labels = \(breaks) '' |> rep(length(breaks)),
    limits = c(-4, 4),
    expand = expansion(),
    name   = NULL
  ) +
  coord_fixed() +
  theme(
    axis.ticks   = element_blank(),
    axis.line    = element_blank(),
    axis.title   = element_markdown(),
    axis.title.y = element_markdown(),
    panel.grid   = element_blank()
  )
```
:::

::: {.column .fragment width="33%"}
```{r question-mark}
fa(
  "question",
  fill           = "#929acb",
  height         = "200px",
  vertical_align = "-10em",
  margin_left    = "4em"
)
```
:::

::: {.column .fragment width="33%"}
```{r corr-traits-transf-example}
#| fig-width:  5.5
#| fig-height: 5.5
```
:::
::::::

¿What is the coordinate system of *θ* to make *θ\** "spherical"?

::: notes
Supongamos una distribución bivariada de rasgos latentes, con una correlación de .5, como la mostrada anteriormente.
Queremos saber cuál es la matriz $\mathbf{P}$ que nos permite pasar de las coordenadas originales, $\mathbf{\theta}$, a las transformadas $\mathbf{\theta^*}$.

Esto es equivalente a preguntar:

\[NEXT\]

¿Cuál es el sistema de coordenadas que representaría adecuadamente los ejes de una distribución bivariada,...

\[NEXT\]

de forma que al transformarlos a coordenadas rectangulares (mediante la matriz $\mathbf{P}$) obtuviésemos una distribución con correlaciones de 0 y varianzas de 1, es decir, una distribución "esférica"?
:::

## Generalization: Oblique axes

```{r create-grid-function}
create_grid <- function(basis,
                        x_limits = c(-4, 4),
                        y_limits = c(-4, 4),
                        break_step = 2) {
  # Argument checking and formatting: ----
    
  ## Basis vectors tangents and norms
  vec_tan <- basis[2, ] / basis[1, ]
  
  
  # Main: ----
  
  # Define grid limits as a function of the axes limits (in canonical metric):
  grid_box <- expand_grid(x = x_limits, y = y_limits) |> mutate(
    x_lim = if_else(x == x_limits[1], "inf", "sup"),
    y_lim = if_else(y == y_limits[1], "inf", "sup")
  )
  
  # Select delimiting grid vertices, based on the basis vector directions
  grid_bounds <- bind_rows(
    h = grid_box |> mutate(slope = vec_tan[1], limit = y_lim),
    v = grid_box |> mutate(slope = vec_tan[2], limit = x_lim),
    .id = "grid"
  ) |>
    filter(xor(slope > 0, x_lim == y_lim)) |>
    mutate(limit_intercept = y - slope * x) # Compute their intercept
  
  # Transformed (1, 1), expressed in the canonical basis
  unit_point <- basis %*% c(1, 1) |> drop() |>
    as_tibble(.name_repair = "minimal") |>
    add_column(name = c('x', 'y') |> paste0('_unit')) |>
    pivot_wider(values_from = value)
  
  # Grid specifications, in canonical basis
  grid_specs <- grid_bounds |>
    bind_cols(unit_point) |>
    mutate(unit_intercept = y_unit - slope * x_unit) |>
    select(-ends_with("unit")) |>
    mutate(
      n_units  = trunc(limit_intercept / unit_intercept),
      n_breaks = n_units %/% break_step * break_step
    )
  
  # Create vertical grid data, if the second basis vector is completely vertical
  if (is.infinite(vec_tan[2])) {
    
    # Create vertical grid data
    v_grid <- grid_bounds |>
      filter(grid == "v") |>
      mutate(
        unit     = basis[1, 1],
        n_units  = trunc(x / unit),
        n_breaks = n_units %/% break_step * break_step
      ) |>
      select(grid, limit, n_breaks, unit) |>
      pivot_wider(names_from = "limit", values_from = n_breaks) |>
      reframe(coord = seq(from = inf, to = sup, by = break_step) * unit) |>
      mutate(
        grid      = 'v',
        slope     = Inf,
        intercept = NA_real_,
        label_pos = coord,
        ref       = as.character(!coord)
      )
    
    # Filter out non-valid grid values
    grid_specs <- grid_specs |> filter(!is.infinite(slope))
  }
  
  # Create grid data
  grid <- grid_specs |>
    select(grid, limit, slope, unit_intercept, n_breaks) |>
    pivot_wider(names_from = limit, values_from = n_breaks) |>
    group_by(grid) |>
    reframe(
      slope,
      coord = seq(from = inf, to = sup, by = break_step),
      intercept = coord * unit_intercept
    ) |>
    mutate(
      label_pos = if_else(
        grid == "h",
         intercept   + x_limits[1] * vec_tan[1],
        (y_limits[1] - intercept)  / vec_tan[2]
      ),
      ref = as.character(!intercept)
    )
  
    if (is.infinite(vec_tan[2])) return(grid |> bind_rows(v_grid))
    
    grid
}

plot_grid <- function(grid,
                      x_limits = c(-4, 4),
                      y_limits = c(-4, 4),
                      break_step = 2,
                      linetype_axis = 'solid',
                      linetype_grid = 'dashed') {
  
  # Argument checking and formatting: ----
    
  ## Grid plot linetypes
  linetypes <- c(`TRUE` = linetype_axis, `FALSE` = linetype_grid)
  
  ## Basis vectors tangents and norms
  vec_tan <- grid |> distinct(grid, slope) |> deframe()
  
  ## Axis title positions
  y_axis_titpos <- (y_limits[2] / vec_tan[2] - x_limits[1]) /
    (x_limits[2] - x_limits[1])
  x_axis_titpos <- (x_limits[2] * vec_tan[1] - y_limits[1]) /
    (y_limits[2] - y_limits[1])
  
  
  # Main: ----
  
  # Assign x axis label values and coordinates (to account for the vertical
  #   special case)
  x_labels <- grid |> filter(grid == "v") |> select(coord, label_pos)
    
  # Create vertical grid geometry if the second vector is completely vertical
  if (is.infinite(vec_tan[2])) {

    geom_vgrid <- geom_vline(
      mapping = aes(xintercept = coord, linetype = ref),
      data = grid |> filter(grid == "v")
    )
    
    grid <- grid |> filter(grid == "h")
  }
  
  grid_plot <- grid |>
    ggplot() +
    geom_abline(
      mapping = aes(
        intercept = intercept,
        slope     = slope,
        linetype  = ref
      )
    )
  
  if (is.infinite(vec_tan[2])) grid_plot <- grid_plot + geom_vgrid
  
  grid_plot +
    scale_x_continuous(
      minor_breaks = NULL,
      breaks = x_labels |> pull(label_pos),
      labels = x_labels |> pull(coord),
      limits = x_limits,
      expand = expansion(),
      name   = NULL,
      sec.axis = dup_axis(name = "*&theta;*~2~", labels = NULL)
    ) +
    scale_y_continuous(
      minor_breaks = NULL,
      breaks = grid |> filter(grid == "h") |> pull(label_pos),
      labels = grid |> filter(grid == "h") |> pull(coord),
      limits = y_limits,
      expand = expansion(),
      name   = NULL,
      sec.axis = dup_axis(name = "*&theta;*~1~", labels = NULL)
    ) +
    scale_linetype_manual(values = linetypes, guide = NULL) +
    coord_fixed() +
    theme(
      axis.ticks   = element_blank(),
      axis.line    = element_blank(),
      axis.title.x       = element_markdown(hjust = y_axis_titpos),
      axis.title.y.right = element_markdown(vjust = x_axis_titpos)
    )
}

transform_grid <- function(basis,
                           x_limits = c(-4, 4),
                           y_limits = c(-4, 4),
                           break_step = 2,
                           linetype_axis = 'solid',
                           linetype_grid = 'dashed') {
  
  # Argument checking and formatting: ----
  
  # Main: ----
  basis |>
    create_grid(
      x_limits   = x_limits,
      y_limits   = y_limits,
      break_step = break_step
    ) |>
    plot_grid(
      x_limits   = x_limits,
      y_limits   = y_limits,
      break_step = break_step,
      linetype_axis = 'solid',
      linetype_grid = 'dashed'
    )
}
```

<br>

:::::: columns
::: {.column width="33%"}
<center>A</center>

```{r grid-orthogonal-rotation}
#| fig-width: 5
rot_basis <- cbind(
  c(sqrt(.75), -sqrt(.25)),
  c(sqrt(.25), sqrt(.75))
)

rot_grid <- transform_grid(rot_basis)

rot_grid
```
:::

::: {.column .fragment width="33%"}
<center>B</center>

```{r grid-ts-basis}
#| fig-width: 5
ts_basis <- cbind(
  c(1, 0),
  c(sqrt(.25), sqrt(.75))
)

ts_grid <- transform_grid(ts_basis)

ts_grid
```
:::

::: {.column .fragment width="33%"}
<center>C</center>

```{r grid-ls-basis}
#| fig-width: 5
ls_basis <- t(solve(ts_basis))

ls_grid <- transform_grid(ls_basis)

ls_grid
```
:::
::::::

::: fragment
[Answers in 3, 2, 1...](https://wall.sli.do/event/dPunB4BMx3S8hfSjaFSuFh?section=ae77dd00-34fb-4600-9677-337a65b04b83)
:::

::: notes
-   ¿Será el A?

\[NEXT\]

-   ¿O puede ser el B?

\[NEXT\]

-   ¿O es el C?

\[NEXT\]

Tomáos vuestro tiempo y en seguida vemos las respuestas.
:::

## Generalization: Oblique axes

<br>

:::::::: columns
::: {.column width="33%"}
<center>A</center>

```{r scatter-orthogonal-rotation}
#| fig-width: 5
thetas_transf <- thetas |>
  group_by(i) |>
  pivot_longer(starts_with("trait")) |>
  mutate(value = rot_basis %*% value |> drop()) |>
  pivot_wider() |>
  ungroup()

rot_grid +
  geom_point(
    data    = thetas_transf,
    mapping = aes(trait_1, trait_2),
    alpha   = .33,
    shape   = 16,
    colour = I(PALETTE_BLUE["normal"])
  )
```
:::

::::: {.column .fragment width="33%"}
<center>B</center>

:::: r-stack
```{r grid-ts-basis}
#| fig-width: 5
```

::: fragment
```{r scatter-ts-basis}
#| fig-width: 5
thetas_transf <- thetas |>
  group_by(i) |>
  pivot_longer(starts_with("trait")) |>
  mutate(value = ts_basis %*% value |> drop()) |>
  pivot_wider() |>
  ungroup()
  
ts_grid +
  geom_point(
    data    = thetas_transf,
    mapping = aes(trait_1, trait_2),
    alpha   = .33,
    shape   = 16,
    colour = I(PALETTE_BLUE["normal"])
  )
```
:::
::::
:::::

::: {.column .fragment width="33%"}
<center>C</center>

```{r scatter-ls-basis}
#| fig-width: 5
thetas_transf <- thetas |>
  group_by(i) |>
  pivot_longer(starts_with("trait")) |>
  mutate(value = ls_basis %*% value |> drop()) |>
  pivot_wider() |>
  ungroup()

ls_scatter <- ls_grid +
  geom_point(
    data    = thetas_transf,
    mapping = aes(trait_1, trait_2),
    alpha   = .33,
    shape   = 16,
    colour = I(PALETTE_BLUE["normal"])
  )

ls_scatter
```
:::
::::::::

::: notes
El primero yo creo que estamos de acuerdo en que no, ya que solamente es una "rotación ortogonal" (como la llamaríamos en el ámbito psicométrico).
La transformación que se aplica podría dar lugar a correlaciones nulas (es lo que haría la transformación de los ejes mediante Análisis de Componentes Principales), pero las varianzas resultantes no serían estandarizadas.

\[NEXT\]

El segundo caso rotaría el eje vertical de manera que ambos ejes formarían un ángulo de 60º, que tiene un coseno igual a 0.5.
La gente que ha pensado que estos son los ejes correctos, pensáis como yo: que el coseno del ángulo entre los ejes representa la correlación entre las dimensiones.
Por lo tanto, cuanto más alta la correlación, más alineados deben estar los ejes.

\[NEXT\]

Sin embargo, mirad lo que ocurre cuando "proyectamos" esa nube de puntos sobre esos ejes.
La "nube de puntos" se achata y se alarga aún más.

\[NEXT\]

La respuesta correcta era la C, que "compensa" de alguna manera el achatamiento de la nube de puntos, de forma que las coordenadas transformadas que nos quedan (en los ejes ortogonales, los "normales" del gráfico) tienen una distribución "esférica" (con matriz de correlaciones "identidad").

Esto, que visto así parece ya evidente, es algo que al menos a mí me ha llevado varias decenas (puede que "cientos") de horas entender, durante al menos dos años trabajando en este problema.
:::

## Latent space vs. test space {.smaller}

> It has been suggested by some researchers that the angle between the axes represents the degree of correlation.
> However, for sake of clarity, an orthogonal axes system is used in which distance measures and vectors can be easily calculated, understood, and interpreted.

---@ackerman_multidimensional_2005-1 [p. 16]

::::::::: columns
::::: column
::: {.fragment fragment-index="1"}
$$
  P(Y_i = 1) = \Phi_L (\mathbf{a}_i^T \mathbf{\theta} + d_i)
$$
:::

::: {.fragment fragment-index="3"}
$$
  P(Y_i = 1) = \Phi_L ({\mathbf{a}^*_i}^T \mathbf{\theta}^* + d_i)
$$
:::
:::::

::::: column
::: {.fragment fragment-index="2"}
$$
  \mathbf{\theta}^* = \mathbf{P} \mathbf{\theta}
$$
:::

::: {.fragment fragment-index="4"}
$$
  \mathbf{a}^*_i = \mathbf{Q} \mathbf{a}_i \\
$$
:::
:::::
:::::::::

::: fragment
$$
  \mathbf{a}_i^T \mathbf{\theta} =
    {\mathbf{a}^*_i}^T \mathbf{\theta}^* =
    \mathbf{a}_i^T \mathbf{Q}^T \mathbf{P} \mathbf{\theta}
$$
:::

::: fragment
$$
  \mathbf{Q} = (\mathbf{P}^{-1})^T
$$
:::

::: notes
Viendo que en la literatura hay afirmaciones como esta, no me resulta totalmente descorazonador.
Parece que no soy el único al que le ha supuesto un esfuerzo entenderlo (resultados de la "encuesta"?).

Pues bien, lo que ocurre en realidad es lo siguiente:

\[NEXT\]

Este es el modelo de TRI multidimensional, recordaréis (ya lo presentamos, es como un modelo de regresión multivariada, donde los predcitores, $\theta$ son variables latentes).
El modelo tiene que ser invariante; es decir, tiene que dar igual qué sistema de coordenadas usemos para representarlo (y esto por cierto también tiene su paralelismo en análisis factorial).

\[NEXT\]

Así que si transformamos los rasgos latentes de las personas a un sistema de coordenadas ortogonal, los ítems tienen que transformarse también.

\[NEXT\]

Pero recordaréis que llamé Q a su matriz de transformación, mientras que a la de los rasgos latentes la llamé P.

\[NEXT\]

Claro, para que se cumpla la invarianza, se tiene que cumplir esta igualdad: Este término debe ser el mismo, da igual el sistema de coordenadas que utilice, y por lo tanto P y Q no pueden ser la misma matriz.

\[NEXT\]

En realidad, para transformar los ítems hay que usar la matriz inversa (inversa traspuesta, más concretamente) de la matriz de transformación de los rasgos latentes.
:::

## Latent space vs. test space {.smaller}

<br>

::::::: columns
::::: column
<center>Espacio latente</center>

:::: r-stack
```{r grid-latent-space}
#| fig-width: 6.5
ls_grid
```

::: fragment
```{r scatter-latent-space}
#| fig-width: 6.5
ls_scatter
```
:::
::::
:::::

::: {.column .fragment}
<center>Espacio test</center>

```{r grid-test-space}
#| fig-width: 6.5
ts_grid
```
:::
:::::::

::: notes
Y esto nos ha permitido "descubrir" (o darnos cuenta, más bien) de que hay dos "espacios vectoriales" distintos, uno en el que se encuentran las coordenadas de rasgo latente (las puntuaciones factoriales),...

\[NEXT\]

...
y otro distinto, que es el espacio en el que se encuentran las coordenadas de los ítems, y que es donde en realidad habría que representarlos.

Porque en este espacio es donde los ángulos entre los ejes son los que verdaderamente son una representación geométrica de las correlaciones entre las variables latentes.
:::

## Latent space vs. test space {.smaller}

```{r space-inner-products}
# Mahalanobis distance:
TRAIT_VECTOR        <- latex_bf("\\theta")
TRAIT_NORM          <- latex_norm(TRAIT_VECTOR)
TRAIT_VECTOR_T      <- latex_transp(TRAIT_VECTOR)
TRAIT_COV_INV_TRAIT <- latex(TRAIT_VECTOR_T, COV_MATRIX_INV, TRAIT_VECTOR)
MAH_DIST_DEF        <- latex_sqrt(TRAIT_COV_INV_TRAIT)
MAH_DIST_EQ         <- latex_eq(TRAIT_NORM, MAH_DIST_DEF)

INNER_PROD_INV_COV_EQ <- latex_eq(COV_MATRIX_INV, INNER_PROD_TRANSF_DEF)

# Latent space:
TRAIT_VECTOR_1    <- latex_sub(TRAIT_VECTOR, 1)
TRAIT_VECTOR_2    <- latex_sub(TRAIT_VECTOR, 2)
TRAIT_VECTOR_1_T  <- latex_transp(TRAIT_VECTOR_1)

INNER_PROD     <- latex_innerprod(TRAIT_VECTOR_1, TRAIT_VECTOR_2)
INNER_PROD_DEF <- latex(TRAIT_VECTOR_1_T, COV_MATRIX_INV, TRAIT_VECTOR_2)
INNER_PROD_EQ  <- latex_eq(INNER_PROD, INNER_PROD_DEF)

# Test space:
DISCR_VECTOR_J <- latex_sub(DISCR_VECTOR_ANY, AUX_INDEX)
DISCR_VECTOR_T <- latex_transp(DISCR_VECTOR)

INNER_PROD_DISCR     <- latex_innerprod(DISCR_VECTOR, DISCR_VECTOR_J)
INNER_PROD_DISCR_DEF <- latex(DISCR_VECTOR_T, COV_MATRIX, DISCR_VECTOR_J)
INNER_PROD_DISCR_EQ  <- latex_eq(INNER_PROD_DISCR, INNER_PROD_DISCR_DEF)
```

<br>

@mahalanobis_reprint_2018:

::::: columns
::: {.column width="50%"}
<center>$`r MAH_DIST_EQ`$</center>
:::

::: {.column .fragment width="50%"}
$`r INNER_PROD_INV_COV_EQ`$
:::
:::::

<br>

::: fragment
> \[...\] the latent space should be regarded as a general Euclidean space with its *inner product* defined by $`r INNER_PROD_EQ`$.

---@zhang_theoretical_1999 [p. 221]
:::

::: fragment
> \[...\] the *inner product* in \[the test space\] is consistently defined by
>
> $`r INNER_PROD_DISCR_EQ`$

---@zhang_theoretical_1999 [p. 221]
:::

::: notes
Esto da lugar a algo muy curioso, y es que la "norma" de un punto en el espacio latente (la longitud de un vector aplicado en el origen que termina en ese punto) viene determinada por la distancia de Mahalanobis (esto es una reimpresión de 1936, no penséis que es una cosa de anteayer).

\[NEXT\]

Y eso se debe a que, para que la transformación en el espacio latente se cumpla, esa matriz de transformación (la que llamababmos P) tiene que ser "raíz cuadrada" de la inversa de la matriz de covarianzas.

\[NEXT\]

Y, curiosamente también, aparece en la literatura, en el año 1999, en un artículo que no tiene nada que ver con esto.
Lo menciona de soslayo y no profundiza en su deducción como hemos hecho aquí.
Solamente lo utiliza para resolver un problema "parcial" que se encuentra en la deducción de un índice de dimensionalidad, distinto de los parámetros de los que estamos hablando.
Pero podía haber obviado este problema y haber utilizado coordenadas ortogonales, sin importarle si había o no correlaciones entre los rasgos latentes, como ha hecho todo el mundo (antes y después que ellos).

Así que, todos mis respetos por estos autores.

Sin embargo claro, ellos estaban hablando de otra cosa, y sólo tocaban este tema de paso.
Así que al final eso se quedó ahí, y nadie ha hecho caso en 25 años a lo que dijeron Zhang y Stout.
La gente ha usado los modelos, hecho simulaciones, etc., usando distribuciones correlacionadas, y ha usado las fórmulas de Reckase para transformar entre parámetros multidimensionales y los parámetros "lineales" (podríamos llamarlos, los de la formulación original) del modelo.

\[NEXT\]

Y lo más importante: Esto nos demuestra efectivamente que tenemos dos espacios distintos: Uno para los parámetros de las personas (espacio latente) y otro para los ítems (espacio test).
Y esto es así porque la operación del producto interno está definida de manera distinta en los dos espacios; si la matriz del producto interno es la inversa de las covarianzas en el espacio latente, entonces en el espacio test, donde la transformación del parámetro de discriminación es inversa (como recordaréis), pues la matriz del producto interno es la matriz de covarianzas.
:::

## Generalized multidimensional parameters

<br>

$$
`r latex_eq(MDISC_COV_PARAM, DISCR_VECTOR_COV_MODULE)`
$$

<br>

::: fragment
$$
`r latex_eq(
     MIL_COV_PARAM,
     latex_curlybraces("$DISTANCE_COV_DEF$, $DIR_COS_ITEM_VEC_COV_DEF$")
   )`
$$
:::

::: notes
Si tenemos en cuenta las correlaciones (más generalmente, covarianzas) de la distribución de puntuaciones factoriales, lo que obtenemos son estas expresiones para los parámetros multidimensionales.

Muy importante, la distancia con signo y la discriminación multidimensional son iguales, no importa en qué espacio (latente, o test) queramos obtenerlas y representarlas.
Pero los cosenos directores es necesario calcularlos en el espacio test, no en el espacio de puntuaciones latentes, como decía Reckase, porque este es el espacio que va a representar en realidad la relación entre las variables latentes.
:::

## Complete graphical representation

```{r item-plot-out}
#| fig-cap:   Orthogonal (left) and oblique (right) test space
#| fig-align: center
#| fig-asp:     .5
#| fig-width: 13

plot_orth <- plot_orth +
  scale_color_manual(values = ITEMS_PALETTE) +
  theme(
    axis.ticks       = element_blank(),
    legend.title     = element_blank(),
    panel.grid.minor = element_blank()
  )

plot_oblique <- plot_oblique +
  scale_color_manual(values = ITEMS_PALETTE, guide = NULL) +
  theme(axis.ticks = element_blank())

plot_orth + plot_oblique ## TODO: Tema ggplot
```

## Conclusions

<br>

Generalization of multidimensional parameters

-   To constant-monotony items

-   To oblique axes

<br>

::: fragment
Distinction latent space - test space
:::

::: notes
En conclusión, lo que hemos propuesto en este artículo consiste en una generalización:

-   A ítems con monotonía constante, creciente, decreciente o nula.
    Esto se venía haciendo ya como hemos visto, pero creemos que hacía falta formalizarlo.
    No basta con hacerlo y punto, si Reckase dijo que las discriminaciones tiene que ser positivas, tenemos que demostrar que también vale para discriminaciones negativas o nulas (monotonía decreciente, o probabilidad constante).

-   A Ejes oblicuos, las cuales consideramos que son necesarias para representar distribuciones de puntuaciones de rasgo correlacionadas, en general.
    Hemos visto que esto ya lo mencionaron Zhang y Stout, hace 25 años, pero no proporcionaron una "deducción" de estos parámetros multidimensionales, cosa que nosotros hacemos.

\[NEXT\]

Por último, es imortante recalcar este "descubrimiento" (o "redescubrimiento", más bien) del espacio test, ya que tenemos que ser muy conscientes de que los ítems y las personas están en espacios vectoriales distintos.
Aunque esto parezca solamente un "artefacto matemático" (y en el fondo lo es), hemos visto que es verdaderamente crucial tenerlo en cuenta para entender bien los modelos que estamos utilizando.
Además, esta "distinción" se halla también en los modelos factoriales: Existe lo que se denominan "ejes primarios" y "ejes de referencia", propuestos por Thurstone hace años (pero que prácticamente no se utilizan ni tienen en consideración).
<!-- # TODO: Hace cuántos años? (Thurstone) --> Sin embargo, cuidado con esto, porque no está claro que esos espacios en TRI multidimensional se puedan asimilar directamente a los ejes en un modelo de AF.
:::

## To be continued...

<br>

-   Multidimensional IRT - FA parallelism

<br>

::: fragment
-   M2PL model extensions: Graded scale, nominal response, 3PL...
:::

<br>

::: fragment
-   Other models
:::

::: notes
Esta de hecho puede ser una futura línea de investigación.
Puede parecer evidente el paralelismo (o al menos a mí me lo parece), pero si una cosa me ha enseñado el estar trabajando en este problema de los parámetros (y toparme con el problema de los dos espacios), es que no hay que fiarse de las apariencias.
Por eso aplicamos el método científico (el método puramente deductivo, en este caso)

\[NEXT\]

El modelo multidimensional logístico de 2 parámetros se puede considerar como "anidado", o un caso más simple de varios modelos.
Así que partiendo de este como base, lo interesante es aplicarlo a otros más complejos, pero más útiles en la práctica.
Por cierto, que esto también lo hace mucha gente, aplica las fórmulas de Reckase a otros modelos y a correr, se quedan "tan anchos".

El modelo de escala graduada, por ejemplo, sería el equivalente al modelo de AF para respuestas ordinales, con varios umbrales o "posiciones" en TRI.

El de crédito parcial, por ejemplo, puede servir también para formatos con varias opciones de respuesta que miden o pesan en factores diferentes cada una de ella.

El logístico de 3 parámetros es como el de 2, pero añadiendo un parámetro de pseudoazar, para modelar el que un participante acierte un ítem por pura suerte, respondiendo "a ciegas" (esto sólo valdría para ítems cognitivos).

Estos modelos, que son "extensiones" de este más básico, deberían partir de estos parámetros como fundamento.
Pero un investigador no debería dar los parámetros sin más de ese modelo, sin proporcionar una prueba formal de que se aplica esa fórmula y se cumplen los supuestos.

\[NEXT\]

Y por supuesto, existen muchos otros modelos psicométricos, muy útiles y aplicados en la investigación en Psicología y Educación, en los cuales no está claro aún cómo generalizar estos resultados al ámbito multidimensional.
:::

## References {.smaller}
