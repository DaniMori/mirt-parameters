---
title: "ObservacionesMario"
author: "Mario Luzardo"
date: "2024-03-12"
output: html_document
---

  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls()) 
library(pspline)
library(mvtnorm)
library(sn)
library(mnormt)
library(scatterplot3d)
library(copula)
library(ggplot2)
library(fungible)
library(pracma)
library(matlib)
library(rgl)
library(MASS)
set.seed(123)
```



# Observación 1

Tenemos que $M=P^TP$ es decir viene a partir de la matriz de cambio de base. 
En nuestro problema tenemos conocidos  los ejes oblicuos dado que ellos representan los rasgos latentes del fenómeno de estudio y por lo tanto la matriz P. 


Pero como saso de estudio consideremos que tenemos $M$ y queremos determinar P y por lo tanto la base oblicua. 
A partir de $M$ no se determina univocamente la matriz $P$ (y por lo tanto la base). Para ver la clase de las matrices $P$ o de cambio de base que son compatibles observemos que si $P_1$ es una matriz de cambio de base que verifica $M=P_1^TP_1$ entonces $P_2=AP_1$ con A matriz ortogonal ($A^T=A^{-1}$) determina la misma matriz de producto interno pues

$$P_2^TP_2=P_1^T A^TAP_1=P_1^T=P_1=M$$
Las rotaciones cumplen con esta propiedad

Esto implica que cualquiera de estas matrices (finalmente bases) determinan la misma matriz de Gramm sin embargo cualitativamente no tienen porque dar cuenta del fenómeno.


## Caso $M=R^{-1}$

Veamos ahora algunas observaciones respecto a la seccion Generalized multidimensional parameters cuando $M=R^{-1}$
  
  Además $M=P^tP$ donde P es una matriz de cambio de base.
Entonces cabe hallar P tal que $R^{-1}=P^tP$
  
  Estamos psrtiendo del hecho que no conocemos la base B pues si la conocemos, tenemos P y por ende M y esta puede o no coincidir con $R^{-1}$ (en general no).

Pondré dos posible formas de hallar P que no son la únicas pero sirven para ejemplificar.

a) via descompisición de Cholesky
b) via descomposición espectral

## Halla P mediante descompisición de Cholesky

Como R es simétrica y supongamos definida positiva entonces por Cholesky podemos escribir

$$R=L L^t$$
  donde L es una matrz triangular inferior, 

$$M=R^{-1}=(L^{-1})^T L^{-1}$$
  
  luego

o sea $P=L^{-1}$
  
  
  
  
## Halla P mediante descompisición de espectral
  
  
  Idem como R es simétrica  y definida positiva por el teorema de la descomposición espectral

$$R=QDQ^T$$
  con Q matriz de vectores propios y D matriz diagonal de valores propios

luego  

$$R=(U \sqrt(D))(\sqrt(D) U)^T$$
  
  
  por lo que $$P=(U\sqrt(D))^{-1}$$
  
  
  Ejemplo

Simulemos datos con matriz de varianza covarianzas

```{r  include=FALSE, echo=FALSE}
S1 <- matrix(c(4.4, 2.5, 2.5, 8), nrow = 2, ncol = 2)
print(S1)
```



```{r  include=FALSE, echo=FALSE}
S1 <- matrix(c(4.4, 2.5, 2.5, 8), nrow = 2, ncol = 2)
# Generar datos normales bivariados
n <- 30000  # Número de observaciones
mu <- c(0, 0)  # Medias de las variables
set.seed(1234567)
data <- mvrnorm(n = n, mu = mu, Sigma = S1)

# Convertir a data frame para visualización
data_df <- as.data.frame(data)
```

Los datos simulados tiene esta matriz $\Sigma$
  
```{r   echo=FALSE}
S1_E=var(data_df)
print(S1_E)
```

y matriz $R$
  ```{r   echo=FALSE}
R1=cor(data_df)
print(R1)
```
```{r   echo=FALSE}
plot(data_df$V1,data_df$V2)
```

Vamos a encontrar dos bases que pueden servir para estos datos y que generan la misma matriz de producto interno.

### Base usando Cholesky queda

```{r   echo=FALSE}
L=t(chol(R1))
P=solve(L)
print(P)
```

Angulo entre los ejes y la base canónica

```{r }
matlib::angle(P[,1],P[,2])
matlib::angle(P[,1],c(1,0))
matlib::angle(P[,2],c(1,0))
matlib::angle(P[,1],c(0,1))
matlib::angle(P[,2],c(0,1))
```

Usando que $$\theta^E=P\theta^B$$ (con E la base canónica) obtenemos las coordenadas en E

```{r   echo=FALSE}
thetaE1=as.matrix(data_df)%*%t(P)
```
Con varianza covarianza y correlaciones

```{r }
var(thetaE1)
cor(thetaE1)
```

Vemos que $\Sigma'$ no es diagonal


### Base usando descomposición espectral
```{r   echo=FALSE}
DE=eigen(R1)
raizD=diag(sqrt(DE$values))
K=DE$vectors%*%raizD
P2=solve(K)
print(P2)
```
Angulo entre los ejes y la base canónica

```{r }
matlib::angle(P2[,1],P2[,2])
matlib::angle(P2[,1],c(1,0))
matlib::angle(P2[,2],c(1,0))
matlib::angle(P2[,1],c(0,1))
matlib::angle(P2[,2],c(0,1))
```
Igual que antes se transforman los datos a la base canónica y se halla la matriz de varianza de los datos transformados

```{r   echo=FALSE}
thetaE2=as.matrix(data_df)%*%t(P2)
print(cov(thetaE2))
print(cor(thetaE2))
```

Igual que antes la matriz Vemos que $\Sigma'$ no es diagonal


## Caso $M=\Sigma ^{-1}$

En este caso $\Sigma ^{-1}=P^TP$ por lo tanto la matriz de varianzas y covarianzas del transformado es

$$Var(\theta^E)=Pvar(\theta^B) P^T=P \Sigma P^T=P P^{-1}(P^T)^{-1}P^T=I$$
Vemos que sto se verifica con los datos simulados


### Base usando Cholesky queda

```{r   echo=FALSE}
LS=t(chol(S1_E))
PS=solve(LS)
print(PS)
```

Angulo entre los ejes y la base canónica

```{r }
matlib::angle(PS[,1],PS[,2])
matlib::angle(PS[,1],c(1,0))
matlib::angle(PS[,2],c(1,0))
matlib::angle(PS[,1],c(0,1))
matlib::angle(PS[,2],c(0,1))
```

Coordenadas en E

```{r   echo=FALSE}
thetaES=as.matrix(data_df)%*%t(PS)
```
Con varianza covarianza y correlaciones

```{r }
var(thetaES)
cor(thetaES)
```

### Base usando descomposición espectral

```{r   echo=FALSE}
DE_S=eigen(S1_E)
raizD_S=diag(sqrt(DE_S$values))
K_S=DE_S$vectors%*%raizD_S
P2_S=solve(K_S)
print(P2_S)
```
Angulo entre los ejes y la base canónica

```{r }
matlib::angle(P2_S[,1],P2_S[,2])
matlib::angle(P2_S[,1],c(1,0))
matlib::angle(P2_S[,2],c(1,0))
matlib::angle(P2_S[,1],c(0,1))
matlib::angle(P2_S[,2],c(0,1))
```
Igual que antes se transforman los datos a la base canónica y se halla la matriz de varianza de los datos transformados

```{r   echo=FALSE}
thetaE2_S=as.matrix(data_df)%*%t(P2_S)
print(cov(thetaE2_S))
print(cor(thetaE2_S))
```



# Observación 2
Este punto para mi es importante pues las secciones anteriores se formulan en base a que conocemos la base oblicua donde el fenómeno es interpretable y tiene las dimensiones "verdaderas".

Entonces si uno tiene las respuestas por ej dicotomicas de los ítems hay una directa relación entre los parámetros y los loadings y comunalidades del análisis factorial 

Por ejemplo en mirt se realiza un analisis factorial (con nfact) donde las variables son los ítems y se calculan los parámetros como

$a_{ij}=\frac{loading_{ij}}{\sqrt(1-h^2)}$
  
  Esto esta computado en el test space



Si usamos una rotación (por ej oblimin) podemos calcular la base en que estan los parametros comparandola con la solucion sin rotar. (PONER FORMULAS)


