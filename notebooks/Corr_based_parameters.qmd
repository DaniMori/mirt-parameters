---
title: "Parámetros basados en la matriz de correlaciones"
format: docx
editor: source
---

# Introducción

Tal y como Mario ha demostrado anteriormente, la versión "basada en la
correlación" de los parámetros multidimensionales no sería correcta, pues no
cumple la premisa de que "la matriz de covarianzas resultante" es diagonal.

No obstante, viendo las derivaciones matemáticas que ha hecho Mario también
respecto a cómo supuestamente obtener la base "verdadera" del espacio test
(en base a la descomposición espectral), creo que se puede encontrar una base
en el espacio test que cumpla esa relación.

# Solución basada en matriz simétrica mediante descomposición espectral

## Supuestos iniciales

Como hemos hecho hasta ahora, asumimos un espacio $\mathbf{\Theta}$ con base
$\mathcal{B}$ y matriz Gramiana del producto interno $\mathbf{M}$ en el que se
representa el vector de rasgo latente $\mathbf{\theta}$. Este vector es una
variable aleatoria $n$-variada que se distribuye con covarianza
$\mathbf{\Sigma} = \mathbf{S} \mathbf{R} \mathbf{S}$, siendo $\mathbf{R}$ una
matriz de correlaciones, semidefinida positiva y con 1's en la diagonal, y
$\mathbf{S}$ una matriz diagonal de desviaciones típicas, tal que
$\sigma^2_{ii} = s^2_{ii}$, para todo $i$ entre 1 y $n$ (i.e., los elementos
diagonales de $\mathbf{\Sigma}$ y $\mathbf{S}^2$ son iguales).

Sea $\mathbf{P}$ la matriz de transformación de la base $\mathcal{B}$ a la base
$\mathcal{U}$. Las coordenadas de $\mathbf{\theta}$, expresadas en la base
$\mathcal{U}$, vienen dadas por la siguiente relación:

$$
\mathbf{\theta}^\mathcal{U} = \mathbf{P} \mathbf{\theta}
$$

Como ya sabemos,
$\mathbf{P} = [\mathbf{b}_1^\mathcal{U}, ..., \mathbf{b}_n^\mathcal{U}]$, es
decir, los vectores de la base $\mathcal{B}$ representados en la base
$\mathcal{U}$. Asumamos además que $\mathcal{U}$ es una base ortogonal.
Sabemos entonces que $\mathbf{P}^T \mathbf{P} = \mathbf{M}$.

## Premisa

Es posible encontrar en el espacio latente $\mathbf{\Theta}$ una matriz de
transformación $\mathbf{P}$ de $\mathcal{B}$ en $\mathcal{U}$ tal
que $\mathbf{\Sigma}^\mathcal{U}$ (i.e. la matriz de covarianzas de
$\mathbf{\theta}$ representado en $\mathcal{U}$, $\mathbf{\theta}^\mathcal{U}$)
sea una matriz diagonal, siendo $\mathbf{M} = \mathbf{R}^{-1}$.

## Demostración

La matriz de covarianzas $\mathbf{\Sigma}^\mathcal{U}$ de
$\mathbf{\theta}^\mathcal{U}$ cumple

$$
\mathbf{\Sigma}^\mathcal{U} = \mathbf{P} \mathbf{\Sigma} \mathbf{P}^T =
  \mathbf{P} \mathbf{S} \mathbf{R} \mathbf{S} \mathbf{P}^T
$$

Supongamos que $\mathbf{P}$ es una raíz cuadrada de $\mathbf{R}^{-1}$; es
decir, $\mathbf{R}^{-1} = \mathbf{P}^T \mathbf{P}$.

$$
\mathbf{\Sigma}^\mathcal{U} =
  \mathbf{P} \mathbf{S} (\mathbf{P}^T \mathbf{P})^{-1} \mathbf{S} \mathbf{P}^T =
  \mathbf{P} \mathbf{S} \mathbf{P}^{-1} (\mathbf{P}^{-1})^T \mathbf{S} \mathbf{P}^T
$$

Si $\mathbf{S}$ conmuta con $\mathbf{P}^{-1}$ y $(\mathbf{P}^{-1})^T$, entonces
la ecuación anterior puede expresarse como

$$
\mathbf{\Sigma}^\mathcal{U} =
  \mathbf{P} \mathbf{P}^{-1} \mathbf{S} \mathbf{S} (\mathbf{P}^{-1})^T \mathbf{P}^T =
  \mathbf{S}^2
$$

### Contraejemplo

Lo anterior en general no se cumple, por lo que $\mathbf{\Sigma}^\mathcal{U}$ no
es, en general, una matriz diagonal bajo estos supuestos.

Un contraejemplo de esto es la solución con la base del ejemplo de
representación gráfica del borrador y varianzas desiguales. Supongamos

$$
\mathbf{S} = diag(\sigma_1, \sigma_2)
$$

$$
(\mathbf{P}^{-1})^T = \begin{pmatrix}
    1 & \rho \\
    0 & \sqrt{1 - \rho^2}
  \end{pmatrix}.
$$

Bajo estos supuestos,

$$
\mathbf{R} = \mathbf{P}^{-1} (\mathbf{P}^{-1})^T = \begin{pmatrix}
       1 &              0 \\
    \rho & \sqrt{1 - \rho^2}
  \end{pmatrix}
  \begin{pmatrix}
    1 & \rho \\
    0 & \sqrt{1 - \rho^2}
  \end{pmatrix} =
  \begin{pmatrix}
       1 & \rho \\
    \rho &    1
  \end{pmatrix}
$$

$$
\mathbf{\Sigma} = diag(\sigma_1, \sigma_2)
  \begin{pmatrix}
       1 & \rho \\
    \rho &    1
  \end{pmatrix}
  diag(\sigma_1, \sigma_2) =
  \begin{pmatrix}
    \sigma_1^2             & \sigma_1 \sigma_2 \rho \\
    \sigma_1 \sigma_2 \rho & \sigma_2^2
  \end{pmatrix}.
$$

Veamos que $\mathbf{S}$ No conmuta con $\mathbf{P}^{-1}$ ni con
$(\mathbf{P}^{-1})^T$:

$$
\mathbf{S} (\mathbf{P}^{-1})^T = diag(\sigma_1, \sigma_2)
  \begin{pmatrix}
    1 & \rho \\
    0 & \sqrt{1 - \rho^2}
  \end{pmatrix} =
  \begin{pmatrix}
    \sigma_1 & \sigma_1 \rho \\
    0        & \sigma_2 \sqrt{1 - \rho^2}
  \end{pmatrix}
$$

$$
(\mathbf{P}^{-1})^T \mathbf{S} = \begin{pmatrix}
    1 & \rho \\
    0 & \sqrt{1 - \rho^2}
  \end{pmatrix}
  diag(\sigma_1, \sigma_2) =
  \begin{pmatrix}
    \sigma_1 & \sigma_2 \rho \\
    0        & \sigma_2 \sqrt{1 - \rho^2}
  \end{pmatrix}
$$

$$
\mathbf{S} \mathbf{P}^{-1} = diag(\sigma_1, \sigma_2)
  \begin{pmatrix}
       1 &              0 \\
    \rho & \sqrt{1 - \rho^2}
  \end{pmatrix} =
  \begin{pmatrix}
    \sigma_1      &              0 \\
    \sigma_2 \rho & \sigma_2 \sqrt{1 - \rho^2}
  \end{pmatrix}
$$

$$
\mathbf{P}^{-1} \mathbf{S} = \begin{pmatrix}
       1 &              0 \\
    \rho & \sqrt{1 - \rho^2}
  \end{pmatrix}
  diag(\sigma_1, \sigma_2) =
  \begin{pmatrix}
    \sigma_1      &              0 \\
    \sigma_1 \rho & \sigma_2 \sqrt{1 - \rho^2}
  \end{pmatrix}
$$

Estas ecuaciones muestran que la conmutatividad de $\mathbf{S}$ con
$\mathbf{P}^{-1}$ y $(\mathbf{P}^{-1})^T$ no se cumple en general (salvo que
$\sigma_1 = \sigma_2$).

### Condiciones adicionales

A pesar de lo anterior, $\mathbf{S}$ conmuta con cualquier otra matriz que sea
simultáneamente diagonalizable con ella. En el caso de matrices simétricas, es
decir, si $\mathbf{P}^{-1} = (\mathbf{P}^{-1})^T$, sabemos que se cumple la
conmutatividad de $\mathbf{S}$ con $\mathbf{P}^{-1}$ (y por lo tanto también con
$(\mathbf{P}^{-1})^T$), si
$\mathbf{S} \mathbf{P}^{-1} = \mathbf{P}^{-1} \mathbf{S}$ es también simétrica.

Intentamos obtener a continuación una solución que haga $\mathbf{P}^{-1}$
simétrica, y comprobamos si $\mathbf{S} \mathbf{P}^{-1}$ es también simétrica.
[^diagonalization]

[^diagonalization]: En realidad, la forma idónea de proceder sería encontrar la
diagonalización simultánea de $\mathbf{S}$ y $\mathbf{P}^{-1}$, ya que si son
simultáneamente diagonalizables entonces son conmutativas. No obstante, esto es
más complejo para mí y ahora mismo no tengo claro saber cómo hacerlo, mientras
que para obtener la $\mathbf{P}^{-1}$ simétrica puedo reaprovechar las
deducciones de Mario mediante la descomposición espectral.

### Solución mediante descomposición espectral de $\mathbf{R}$

Puesto que $\mathbf{R}$ es semidefinida positiva, existen las matrices
$\mathbf{Q}$ y $\mathbf{\Lambda}$ tal que
$\mathbf{R} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T$, y además:

* $\mathbf{Q}$ es una matriz ortonormal, i.e., $\mathbf{Q}^T = \mathbf{Q}^{-1}$.
* $\mathbf{\Lambda}$ es una matriz diagonal.

Dado que $\mathbf{\Lambda}$ es diagonal, aplicamos
$\mathbf{\Lambda} = \mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2}$ para obtener
$\mathbf{P}^{-1}$ y $(\mathbf{P}^{-1})^T$:

$$
\mathbf{R} = \mathbf{P}^{-1} (\mathbf{P}^{-1})^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
$$

Ahora bien, para obtener una $\mathbf{P}^{-1}$ simétrica, podemos aplicar

$$
\mathbf{P}^{-1} (\mathbf{P}^{-1})^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^{-1}
    \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
    \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
$$

Igualando $\mathbf{P}^{-1} = \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T$,
tenemos que
$(\mathbf{P}^{-1})^T = \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T$
y por tanto $\mathbf{P}^{-1} = (\mathbf{P}^{-1})^T$. Luego 
$\mathbf{P}^{-1} = \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T$ es una
solución que hace a $\mathbf{P}^{-1}$ simétrica.

### Comprobación de la simetría de $\mathbf{S} \mathbf{P}^{-1}$

Comprobamos si el producto $\mathbf{S} \mathbf{P}^{-1}$ da lugar a una matriz
simétrica.

$$
\mathbf{S} \mathbf{P}^{-1} =
  \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
$$

$$
(\mathbf{S} \mathbf{P}^{-1})^T =
  (\mathbf{P}^{-1})^T \mathbf{S}^T = \mathbf{P}^{-1} \mathbf{S}
$$

$$
(\mathbf{S} \mathbf{P}^{-1})^T =
  (\mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T)^T =
  (\mathbf{Q}^T)^T (\mathbf{\Lambda}^{1/2})^T \mathbf{Q}^T \mathbf{S}^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T \mathbf{S}
$$

$$
\mathbf{P}^{-1} \mathbf{S} =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T \mathbf{S}
$$

Según este resultado,
$(\mathbf{S} \mathbf{P}^{-1})^T = \mathbf{S} \mathbf{P}^{-1}$, pero esto no
demuestra que $\mathbf{S} \mathbf{P}^{-1}$ sea simétrica ni que $\mathbf{S}$ y
$\mathbf{P}^{-1}$ sean conmutativas (ver
[aquí un contraejemplo](https://math.vanderbilt.edu/sapirmv/msapir/jan22.html)
).

Podemos utilizar las siguientes propiedades: Sabemos que $\mathbf{S}$ y
$\mathbf{P}$ son semidefinidas positivas[^P_SDP]. Dado que $\mathbf{P}$ es
semidefinida positiva, $\mathbf{P}^{-1}$ también lo es. Según
[Meensakshi & Rakian, 1999](https://core.ac.uk/download/pdf/82822897.pdf)
(referenciado en
[esta respuesta en Math StackExchange](https://math.stackexchange.com/a/483930)
), la condición para que $\mathbf{S} \mathbf{P}^{-1}$ sea simétrica es que
$\mathbf{S} \mathbf{P}^{-1}$ sea "normal", es decir, que
$\mathbf{S} \mathbf{P}^{-1} (\mathbf{S} \mathbf{P}^{-1})^T =
  (\mathbf{S} \mathbf{P}^{-1})^T \mathbf{S} \mathbf{P}^{-1}$.
Ahora bien,

[^P_SDP]: Estoy asumiendo que $\mathbf{P}$ es semidefinida positiva porque
$\mathbf{R}$ lo es, y $\mathbf{P}T \mathbf{P} = \mathbf{R}$. Puede que haya un
salto inferencial aquí (i.e., que sea necesario demostrar que $\mathbf{P}$ es
semidefinida positiva).

$$
\mathbf{S} \mathbf{P}^{-1} (\mathbf{S} \mathbf{P}^{-1})^T =
  \mathbf{S} \mathbf{P}^{-1} (\mathbf{P}^{-1})^T \mathbf{S}^T =
  \mathbf{S} \mathbf{P}^{-1} (\mathbf{P}^{-1})^T \mathbf{S} =
  \mathbf{\Sigma}.
$$

Como $\mathbf{\Sigma}$ es simétrica (y semidefinida positiva),
el producto $\mathbf{S} \mathbf{P}^{-1}$ es conmutativo. Comprobamos no obstante
la propiedad de "normalidad" de $\mathbf{S} \mathbf{P}^{-1}$:

$$
\mathbf{S} \mathbf{P}^{-1} (\mathbf{S} \mathbf{P}^{-1})^T =
  \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
    (\mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T)^T =
  \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
    (\mathbf{Q}^T)^T \mathbf{\Lambda}^{1/2}^T \mathbf{Q}^T \mathbf{S}^T =
  \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
    \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T \mathbf{S} =
  \mathbf{S} \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T \mathbf{S} =
  \mathbf{S} \mathbf{R} \mathbf{S} =
  \mathbf{\Sigma}
$$

$$
(\mathbf{S} \mathbf{P}^{-1})^T \mathbf{S} \mathbf{P}^{-1} =
  (\mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T)^T
    \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T =
  (\mathbf{Q}^T)^T \mathbf{\Lambda}^{1/2}^T \mathbf{Q}^T \mathbf{S}^T
    \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T \mathbf{S}
    \mathbf{S} \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T =
  \mathbf{P}^{-1} \mathbf{S}^2 (\mathbf{P}^{-1})^T
$$

Esto no parece fácil de demostrar, por desgracia. La ecuación anterior, sin
embargo, nos permite aplicar el resultado al que queríamos llegar, ya que
obtenemos que la distribución de rasgos latentes transformada,
$\mathbf{\theta}^\mathcal{E}$, tiene por matriz de covarianzas $\mathbf{S}^2$.

$$
\mathbf{\Sigma}^\mathcal{E} = \mathbf{P} \mathbf{S}^2 \mathbf{P}^T
  \mathbf{P} \mathbf{P}^{-1} \mathbf{S}^2 (\mathbf{P}^{-1})^T \mathbf{P}^T =
  \mathbf{S}^2
$$

## Contraejemplo

Lo anterior parece contener razonamientos circulares, por lo que no está claro
que sea correcto. Vamos a probar con uno de los ejemplos que sabemos que pueden
ser problemáticos, es decir, cuando las varianzas son desiguales.

```{r 2d-example}
corr_matrix <- c(1, .5, .5, 1) |> matrix(ncol = 2)
std_devs    <- c(2, 3) |> diag()
cov_matrix  <- std_devs %*% corr_matrix %*% std_devs

corr_matrix
std_devs

cov_matrix <- std_devs %*% corr_matrix %*% std_devs
cov_matrix


eigendecomposition <- corr_matrix |> eigen()
lambda             <- eigendecomposition$values
Q                  <- eigendecomposition$vectors

# Base calculada:
lambda_sqrt <- eigendecomposition$values |> sqrt() |> diag()

P_inv <- Q %*% lambda_sqrt %*% solve(Q)

# Comprobación de la simetría y conmutatividad:
std_devs %*% P_inv # No es simétrica
P_inv %*% std_devs # No es conmutativa
```

Vemos que, en general, no se cumplen las condiciones de simetría y
conmutatividad, por lo que la solución no es correct.

## Demostración de la falta de solución

En general, hay que considerar que estamos partiendo de la premisa de que
$\mathbf{P} \mathbf{R} \mathbf{P}^T = \mathbf{I}$ y que
$\mathbf{P} \mathbf{\Sigma} \mathbf{P}^T$ es una matriz diagonal.

Por lo tanto, estamos asumiendo que $\mathbf{\Sigma}$ y $\mathbf{R}$ son
simultáneamente diagonalizables. Si atendemos a esta respuesta en [Mathematics
StackExchange](https://math.stackexchange.com/a/1580856) (y asumimos que es
correcta), vemos que para que dos matrices sean simultáneamente diagonalizables
han de ser conmutativas.

Dado que $\mathbf{\Sigma} = \mathbf{S} \mathbf{R} \mathbf{S}$,

$$
\mathbf{\Sigma} \mathbf{R} = \mathbf{S} \mathbf{R} \mathbf{S} \mathbf{R}
$$

$$
\mathbf{R} \mathbf{\Sigma} = \mathbf{R} \mathbf{S} \mathbf{R} \mathbf{S}
$$

Es decir, que $\mathbf{\Sigma}$ y $\mathbf{R}$ son conmutativas si $\mathbf{S}$
y $\mathbf{R}$ lo son. Pero ya hemos visto varios contraejemplos, e.g.:

```{r non-commutativity-corr-sd}
corr_matrix %*% std_devs
std_devs %*% corr_matrix
```

# Aproximación alternativa

Tras ver que no hay solución basada en encontrar una matriz $\mathbf{P}$
simétrica, tal y como esperaba, planteo el siguiente nuevo enfoque para la
versión basada en la matriz de correlaciones:

La versión de los índices basada en la covarianza tiene el problema de que los
parámetros de discriminación "marginales" (i.e. los $a_{ik}$) no tienen escala
invariante. Concretamente, si un ítem mide solamente la dimeensión $k$,
$MDISC_\mathbf{\Sigma} = \sigma_k a_{ik}$, en lugar de $a_{ik}$.

Se puede justificar el uso de $\mathbf{R}$ buscando una solución que cumpla esa
propiedad, pero teniendo en cuenta (parcialmente) la estructura del espacio
latente.

Supongamos una matriz $\mathbf{K}$ tal que
$\mathbf{K} \mathbf{\Sigma} \mathbf{K}^T = \mathbf{I}$, es decir, llamamos a la
raíz de $\mathbf{\Sigma}^-1$ $\mathbf{K}$ en lugar de $\mathbf{P}$.

Supongamos que $\mathbf{K} = \mathbf{P} \mathbf{S}^{-1}$. Tenemos entonces que

$$
\mathbf{P} \mathbf{S}^{-1} \mathbf{\Sigma} \mathbf{S}^{-1} \mathbf{P}^T =
  \mathbf{P} \mathbf{S}^{-1}
    \mathbf{S} \mathbf{R} \mathbf{S}
    \mathbf{S}^{-1} \mathbf{P}^T =
  \mathbf{P} \mathbf{R} \mathbf{P}^T
$$

Por lo tanto,
$\mathbf{R} = (\mathbf{P}^{-1})^T \mathbf{P}^{-1} = \mathbf{M}^{-1}$.

Por otro lado,

$$
\mathbf{\Sigma} = \mathbf{S} (\mathbf{P}^{-1})^T \mathbf{P}^{-1} \mathbf{S}
$$

La matriz de covarianzas transformada es, simplemente,

$$
\mathbf{\Sigma}^\mathcal{U} =
  \mathbf{P} \mathbf{S}
    (\mathbf{P}^{-1})^T \mathbf{P}^{-1}
    \mathbf{S} \mathbf{P}^T
$$

lo cual no admite simplificación (y tendrá la matriz de correlaciones que le
corresponda, la cual en general no será una matriz identidad).

En base a esta propuesta, se pueden justificar los parámetros basados en la
correlación, pero sin llegar a ninguna solución que tenga "propiedades
deseables" desde el punto de vista del espacio latente. No obstante, sí tiene
propiedades deseables desde el punto de vista de los parámetros
multidimensionales, ya que cumple la "propiedad 1" de Reckase. Luego se puede
justificar a partir del incumplimiento de dicha propiedad por parte de la
versión "basada en la covarianza".

## Ejemplo

Veamos cómo quedan las matrices en el ejemplo anterior. Al no haber definido
ninguna propiedad en específico para la matriz de transformación, nos vale
cualquiera, por lo que utilizaremos el ejemplo calculado anteriormente con la
descomposición espectral.

```{r 2d-example-correlation}
# Comprobación: matriz de correlaciones

P_inv %*% t(P_inv) # Raíz de la correlación

P <- P_inv |> solve()
P %*% corr_matrix %*% t(P) |> round(3) # Matriz identidad

# Comprobación: matriz de covarianzas

K <- P %*% solve(std_devs)
K_inv <- K |> solve()

K_inv %*% t(K_inv) # Raíz de la covarianza


K %*% cov_matrix %*% t(K) |> round(3) # Matriz identidad

# Resultado: rasgos latentes transformados
cov_transf <- P %*% cov_matrix %*% t(P)
cov_transf # Covarianza
cov_transf |> cov2cor() # Correlación
```
