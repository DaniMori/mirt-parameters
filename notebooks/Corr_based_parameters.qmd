---
title: "Parámetros basados en la matriz de correlaciones"
format: docx
editor: source
---

# Introducción

Tal y como Mario ha demostrado anteriormente, la versión "basada en la
correlación" de los parámetros multidimensionales no sería correcta, pues no
cumple la premisa de que "la matriz de covarianzas resultante" es diagonal.

No obstante, viendo las derivaciones matemáticas que ha hecho Mario también
respecto a cómo supuestamente obtener la base "verdadera" del espacio test
(en base a la descomposición espectral), creo que se puede encontrar una base
en el espacio test que cumpla esa relación.

# Deducción

## Supuestos iniciales

Como hemos hecho hasta ahora, asumimos un espacio $\mathbf{\Theta}$ con base
$\mathcal{B}$ y matriz Gramiana del producto interno $\mathbf{M}$ en el que se
representa el vector de rasgo latente $\mathbf{\theta}$. Este vector es una
variable aleatoria $n$-variada que se distribuye con covarianza
$\mathbf{\Sigma} = \mathbf{S} \mathbf{R} \mathbf{S}$, siendo $\mathbf{R}$ una
matriz de correlaciones, semidefinida positiva y con 1's en la diagonal, y
$\mathbf{S}$ una matriz diagonal de desviaciones típicas, tal que
$\sigma^2_{ii} = s^2_{ii}$, para todo $i$ entre 1 y $n$ (i.e., los elementos
diagonales de $\mathbf{\Sigma}$ y $\mathbf{S}^2$ son iguales).

Sea $\mathbf{P}$ la matriz de transformación de la base $\mathcal{B}$ a la base
$\mathcal{U}$. Las coordenadas de $\mathbf{\theta}$, expresadas en la base
$\mathcal{U}$, vienen dadas por la siguiente relación:

$$
\mathbf{\theta}^\mathcal{U} = \mathbf{P} \mathbf{\theta}
$$

Como ya sabemos,
$\mathbf{P} = [\mathbf{b}_1^\mathcal{U}, ..., \mathbf{b}_n^\mathcal{U}]$, es
decir, los vectores de la base $\mathcal{B}$ representados en la base
$\mathcal{U}$. Asumamos además que $\mathcal{U}$ es una base ortogonal,
con producto interno estándar. Sabemos entonces que
$\mathbf{P}^T \mathbf{P} = \mathbf{M}$[^orth_basis].

[^orth_basis]: Problema: Esto puede **no ser cierto**, ya que para que ello sea
así, la base $\mathcal{U}$ debe ser ortonormal, y no solamente ortogonal.

## Premisa

Es posible encontrar en el espacio latente $\mathbf{\Theta}$ una matriz de
transformación $\mathbf{P}$ de $\mathcal{B}$ en $\mathcal{U}$ tal
que $\mathbf{\Sigma}^\mathcal{U}$ (i.e. la matriz de covarianzas de
$\mathbf{\theta}$ representado en $\mathcal{U}$, $\mathbf{\theta}^\mathcal{U}$)
sea una matriz diagonal, siendo $\mathbf{M} = \mathbf{R}^{-1}$.

Además, esta matriz invariante en "escala"; es decir, conservará las "varianzas"
de $\mathbf{\Theta}$: $\mathbf{\Sigma}^\mathcal{U} = \mathbf{S}^2$.

## Demostración

La matriz de covarianzas $\mathbf{\Sigma}^\mathcal{U}$ de
$\mathbf{\theta}^\mathcal{U}$ cumple

$$
\mathbf{\Sigma}^\mathcal{U} = \mathbf{P} \mathbf{\Sigma} \mathbf{P}^T.
$$

Por lo tanto, igualamos

$$
\mathbf{S}^2 = \mathbf{P} \mathbf{\Sigma} \mathbf{P}^T
$$

y, operando, obtenemos

$$
\mathbf{P}^{-1} \mathbf{S}^2 (\mathbf{P}^{-1})^T = \mathbf{\Sigma}.
$$

Como $\mathbf{S}$ es diagonal,

$$
\mathbf{S} (\mathbf{P}^T)^{-1} (\mathbf{P}^T)^{-1} \mathbf{S} =
  \mathbf{\Sigma} =
  \mathbf{S} \mathbf{R} \mathbf{S},
$$

Dado que $\mathbf{P}$ es una raíz cuadrada de $\mathbf{R}^{-1}$,
$\mathbf{R} = (\mathbf{P}^T \mathbf{P})^{-1}$.


$$
(\mathbf{P}^T \mathbf{P}^T)^{-1} = (\mathbf{P}^T \mathbf{P})^{-1}.
$$

Es decir, la solución que buscamos es una matriz $\mathbf{P}$, raíz cuadrada de
$\mathbf{R}^{-1}$, que
cumpla la condición de simetría (i.e. $\mathbf{P} = \mathbf{P}^T$).

### Solución mediante descomposición espectral de $\mathbf{R}$

Puesto que $\mathbf{R}^{-1}$ es semidefinida positiva, existen las matrices
$\mathbf{Q}$ y $\mathbf{\Lambda}$ tal que
$\mathbf{R}^{-1} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T$, y además:

* $\mathbf{Q}$ es una matriz ortonormal, i.e., $\mathbf{Q}^T = \mathbf{Q}^{-1}$.
* $\mathbf{\Lambda}$ es una matriz diagonal.

Dado que $\mathbf{\Lambda}$ es diagonal, aplicamos
$\mathbf{\Lambda} = \mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2}$:

$$
\mathbf{R}^{-1} = \mathbf{P}^T \mathbf{P} =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
$$

Igualmente, aplicamos

$$
\mathbf{P}^T \mathbf{P} =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^{-1}
    \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T =
  \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
    \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T
$$

Igualando $\mathbf{P} = \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T$,
tenemos que
$\mathbf{P}^T = \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^T$,
y por tanto $\mathbf{P} = \mathbf{P}^T$.

# Ejemplos

## Ejemplo en $\mathbb{R}^2$

Usamos el ejemplo con la correlación del borrador, y covarianzas de 2 y 3.

```{r 2-dim-example}
corr_matrix <- c(1, .5, .5, 1) |> matrix(ncol = 2)
std_devs    <- c(2, 3) |> sqrt() |> diag()
cov_matrix  <- std_devs %*% corr_matrix %*% std_devs

## Comprobación:
cov_matrix |> cov2cor()
```


```{r basis-computation}
# descomposición espectral de Sigma_U:
eigendecomposition <- corr_matrix |> solve() |> eigen()
lambda             <- eigendecomposition$values
Q                  <- eigendecomposition$vectors

# Base calculada:
lambda_sqrt <- eigendecomposition$values |> sqrt() |> diag()
Q_inv       <- Q |> t()

# Calculo de P por descomposicion espectral
P <- Q %*% lambda_sqrt %*% Q_inv

# Comprobación

## Raíz de la inversa de la correlación:
solve(t(P) %*% P)

## Covarianza en la base transformada:
P %*% cov_matrix %*% t(P)
```

## Ejemplo en $\mathbb{R}^3$

Usamos el ejemplo propuesto por Mario.

```{r 3-dim-example}
corr_matrix <- c(1, .5, .8, .5, 1, .6, .8, .6, 1) |> matrix(ncol = 3)
std_devs    <- c(2, 5, 3) |> diag()
cov_matrix  <- std_devs %*% corr_matrix %*% std_devs
```

```{r basis-computation}
```
